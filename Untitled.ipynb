{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "detected-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files  压缩文件的路径\n",
    "    :param database_name: Name of database   数据集的名称\n",
    "    :param data_path: Path to extract to  解压的路径\n",
    "    :param _: HACK - Used to have to same interface as _ungzip      \n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))    \n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "'''\n",
    "with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行\n",
    "必要的“清理”操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。\n",
    "\n",
    "类似于while  \n",
    "'''\n",
    "\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database   \n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)  #os.makedirs() 方法用于递归创建目录\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(   #urlretrieve()方法直接将远程数据下载到本地\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)  #解压\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading   #显示文件下载进度\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "configured-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ml-1m Data\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'   #./当前目录   ../上级目录\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subsequent-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()          # 读取文件  以\\t为分割符  第一行为头文件    \n",
    " #返回data的前几行数据，默认为前五行，需要前十行则data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "limiting-laser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floral-slovak",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "embedded-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')  #正则化过滤\n",
    "    users_orig = users.values     #原始数据\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)   #将性别映射成01\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}     \n",
    "    #set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。\n",
    "    #enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。\n",
    "      #这里的ii是索引值，val是真正的列表中Title元素\n",
    "\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    \n",
    "    \n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    " #re.compile(strPattern[, flag]):把正则表达式的模式和标识转化成正则表达式对象。供match()和search()这两个函数使用\n",
    "#第二个参数flag是匹配模式，取值可以使用按位或运算符'|'表示同时生效\n",
    "#r表示后面是一个正则表达式''\n",
    "                                #^匹配开头,$匹配结尾,(.*)中的()表示匹配其中的任意正则表达式,.匹配任何字符,*代表可以重复0次或多次\n",
    "                            #\\(和\\)：表示对括号的转义，匹配文本中真正的括号\n",
    "                                #(\\d+)表示匹配()内的任意字符,\\d表示任何数字,+代表数字重复一次或者多次\n",
    "\n",
    "    #pattern.match(val)使用Pattern匹配文本val，获得匹配结果，无法匹配时将返回None\n",
    "     #group获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回，分组是按照()匹配顺序进行\n",
    "     #这里group(1)相当于只返回第一组，分组标号从1开始。不填则为返回全部结果\n",
    "     #这里即完成了将电影名称的时间去掉\n",
    "     # group(1) 相当于对所有电影名称中出现的每个单词进行一列表的组合\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "     #for ii,val in enumerate(set(movies['Title']))得到ii索引值和其对应的不重复的一个电影字符串val(去掉月份的)\n",
    "     #val.split()得到全部被空格分开的电影名称字符串列表，row遍历电影集中一个电影的全部单词\n",
    "     #title_map得到的是字典，格式为'一个电影字符串：[描述这个电影的全部单词构成的一个对应的数值列表]'\n",
    "\n",
    "\n",
    "                \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()   #set() 函数创建一个无序不重复元素集,返回一个可迭代对象\n",
    "    for val in movies['Genres'].str.split('|'):         # 将数据集中的每种电影类型根据|分开 得到一个电影类型的表\n",
    "        genres_set.update(val)        #添加新元素到集合当中，即完成出现电影中的新单词时，存下来    首先是个空表 每出现一个新单词，更新表\n",
    " \n",
    "\n",
    "    genres_set.add('<PAD>')         #全部收录后  添加一个空\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}  #为全部单词进行像字典一样进行标注'描述电影的word：数字'格式,即数字字典\n",
    "                                                                   #而一个电影由多个word构成\n",
    "\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "     #for ii,val in enumerate(set(movies['Title']))得到ii索引值和其对应的不重复的一个电影字符串val\n",
    "      #val.split()得到全部被空格分开的电影名称字符串列表，row遍历电影集中一个电影的全部单词\n",
    "  \n",
    "\n",
    "   #针对映射表的修改\n",
    "    for key in genres_map:   # values() 方法返回一个迭代器，可以使用 list() 来转换为列表，列表为字典中的所有值。\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "                                     #index                    ,object  电影key长度少于15就加填充符  \n",
    "                                     # 具体到二维张量的（a，b)坐标后添加空格\n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():  #根据空格分开\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    #for ii,val in enumerate(set(movies['Title']))得到ii索引值和其对应的不重复的一个电影字符串val(去掉月份的)\n",
    "                                #val.split()得到全部被空格分开的电影名称字符串列表，row遍历电影集中一个电影的全部单词   row对应的具体的电影元素\n",
    "                                #title_map得到的是字典，格式为'一个电影字符串：[描述这个电影的全部单词构成的一个对应的数值列表]'\n",
    "\n",
    "            \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)   #title字段的去掉名称后的电影名转化为对应的数字列表\n",
    "                                            #如电影集中的一行数据如下movieid，title，genre\n",
    "\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "     #合并左dataframe和右datafram，默认为取交集，取交集作为索引键\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]    #删除rating这一列\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outside-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))\n",
    "\n",
    "#序列化对象，将对象obj保存到文件file中去。参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。file表示保存到的类文件对象，file必须有write()接口，file可以是一个以'w'打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lyric-growing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "egyptian-variance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[774, 1517, 1333, 1333, 1333, 1333, 1333, 1333...</td>\n",
       "      <td>[15, 1, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[199, 1333, 1333, 1333, 1333, 1333, 1333, 1333...</td>\n",
       "      <td>[2, 1, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2421, 2889, 3249, 1333, 1333, 1333, 1333, 133...</td>\n",
       "      <td>[3, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[2294, 2183, 3092, 1333, 1333, 1333, 1333, 133...</td>\n",
       "      <td>[3, 18, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1217, 550, 142, 4047, 1195, 874, 1333, 1333, ...</td>\n",
       "      <td>[3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [774, 1517, 1333, 1333, 1333, 1333, 1333, 1333...   \n",
       "1        2  [199, 1333, 1333, 1333, 1333, 1333, 1333, 1333...   \n",
       "2        3  [2421, 2889, 3249, 1333, 1333, 1333, 1333, 133...   \n",
       "3        4  [2294, 2183, 3092, 1333, 1333, 1333, 1333, 133...   \n",
       "4        5  [1217, 550, 142, 4047, 1195, 874, 1333, 1333, ...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [15, 1, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
       "1  [2, 1, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
       "2  [3, 9, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
       "3  [3, 18, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "4  [3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "binding-raise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([774, 1517, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333]),\n",
       "       list([15, 1, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "brutal-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proud-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "planned-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "\n",
    "\n",
    "  #features.take(0,1)得到userid的全部列，由于从0开始编号，则max取最大值再加1可以得到用户id个数\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "loving-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs   epoch：1个epoch表示过了1遍训练集中的所有样本。\n",
    "num_epochs = 5\n",
    "# Batch Size     batch-size：1次迭代所使用的样本量\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reliable-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")  #这里一行代表一个用户的id，是batch×1，每一行是一个列表\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    '''\n",
    "    \n",
    "tf.placeholder(\n",
    "    dtype,\n",
    "    shape=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dtype：数据类型。常用的是tf.float32,tf.float64等数值类型\n",
    "shape：数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）\n",
    "name：名称\n",
    "形参，用于定义过程，在执行的时候再赋具体的值\n",
    "'''\n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob\n",
    "#避免反复地切换底层程序实际运行的上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nuclear-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):     #用户嵌入矩阵输入4个\n",
    "    with tf.name_scope(\"user_embedding\"):  #用于后面tensorboard可视化图层关系\n",
    "        \n",
    "        \n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\") \n",
    "                            #tf.Variable(initializer,name)：initializer是初始化参数,name是变量的名字\n",
    "                            #tf.random_uniform(shape, minval=0,maxval=None,dtype=tf.float32) 从均匀分布中输出随机值。  \n",
    "                            #返回shape形状矩阵：用户数×特征数，产生于low(-1)和high(1)之间，产生的值是均匀分布的。\n",
    "\n",
    "                    \n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "                            #tf.nn.embedding_lookup(tensor, id) 选取一个张量tensor里面索引id对应的元素\n",
    "                            #选取uid_embed_matrix的用户id对应的某个用户id的向量    得到userid对应的张量\n",
    "                             \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "        \n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mechanical-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "         # tf.layers.dense(inputs,units,activation=None,use_bias=True,kernel_initializer=None,bias_initializer=tf.zeros_initializer(),\n",
    "         # kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,\n",
    "         # bias_constraint=None,trainable=True,name=None,reuse=None)\n",
    "         # inputs:该层的输入; units:输出的大小(维数),整数或long; activation: 使用什么激活函数（神经网络的非线性层），默认为None，不使用激活函数\n",
    "         # name该层的名字\n",
    "         #tf.nn.relu: 线性整流函数（Rectified Linear Unit, ReLU），又称修正线性单元。   小于0的时候输出为0 , 大于0输出不变\n",
    "        \n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        #tf.concat(values,axis,name)是连接两个矩阵的操作，本身不会增加维度，返回的是连接后的tensor.\n",
    "        #values应该是一个tensor的list或者tuple。\n",
    "        #axis则是我们想要连接的维度。对于二维来说0表示第一个括号维度，1表示第二个括号维度\n",
    "        \n",
    "        \n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "         #tf.contrib.layers.fully_connected（F输入, num_outputs,activation_fn）用于构建全连接层  双曲正切曲线\n",
    "\n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "        \n",
    "    return user_combine_layer , user_combine_layer_flat\n",
    "\n",
    "#inputs：输入该网络层的数据\n",
    "\n",
    "#units：输出的维度大小，改变inputs的最后一维\n",
    "\n",
    "#activation：激活函数，即神经网络的非线性变化\n",
    "\n",
    "#axis=2 在第二个维度拼接\n",
    "\n",
    "#tf.concat()拼接张量\n",
    "\n",
    "# tf.contrib.layers.fully_connected（F  (输入 , num_outputs  （输出 ,activation_fn  （采用指定的非线性激励函数）\n",
    "\n",
    "#tf.reshape(A,B) 将张量A的形状变成B  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "statewide-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "double-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)  #保持维度\n",
    "                                             #压缩求和，用于降维            \n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adjacent-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1) #维度增加一维\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            # 初始化卷积核参数\n",
    "            # tf.truncated_normal([window_size  窗口大小, embed_dim 嵌入矩阵维度  , 1, filter_num   过滤器大小]   ,stddev=0.1)   从截断的正态分布中输出随机值\n",
    "            #得到正态分布输出为shape，mean均值，stddev标准差\n",
    "            \n",
    "           \n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            #创建一个常数张量      偏差项  防止过拟合    这里的shape表示(8,)\n",
    "            \n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            #卷积                     指需要做卷积的输入图像 filter      相当于CNN中的卷积核         移动距离            边界填充 无    \n",
    "            \n",
    "            \n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "               #tf.nn.relu()函数的目的是，将输入小于0的值幅值为0，输入大于0的值不变\n",
    "               #一个叫bias的向量加到一个叫value的矩阵上，是向量与矩阵的每一行进行相加，得到的结果和value矩阵大小相同。 无关项\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            \n",
    "            #最大池化                       输入的矩阵      【】池化窗口                           池化步长\n",
    "            \n",
    "            \n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "             \n",
    "                                \n",
    "            #append() 方法用于在列表末尾添加新的对象。\n",
    "            #这里最后得到4个tensor堆叠的列表\n",
    "            #[<tf.Tensor 'movie_txt_conv_maxpool_2/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
    "            #<tf.Tensor 'movie_txt_conv_maxpool_3/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
    "            #<tf.Tensor 'movie_txt_conv_maxpool_4/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>, \n",
    "            #<tf.Tensor 'movie_txt_conv_maxpool_5/maxpool_layer:0' shape=(?, 1, 1, 8) dtype=float32>]\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    #Dropout层     \n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        # 在第三个维度拼接\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "        #                                  -1 变成标量  变成一列\n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "        #tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)\n",
    "        #x：输入;keep_prob：保留比例,取值 (0,1],每一个参数都将按这个比例随机变更\n",
    "        #dropout是CNN中防止过拟合的一个trick\n",
    "        #dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。\n",
    "        #注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络\n",
    "\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ideal-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "imported-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#变量常量等等基本量的操作设置完成，意味着最基本的东西都有了，然后接下来很重要的就是那些量和操作怎么组成更大的集合，怎么运行这个集合\n",
    "#这些就是计算图谱Graph和Session的作用：（参见https://blog.csdn.net/xierhacker/article/details/53860379）\n",
    "#一、graph\n",
    "#一个TensorFlow的运算，被表示为一个数据流的图。 \n",
    "#一幅图中包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点\n",
    "#一旦开始任务，就已经有一个默认的图创建好了。可以通过调用tf.get_default_graph()来访问。添加一个操作到默认的图里面，只需调用一个定义了新操作的函数\n",
    "#另外一种典型用法是要使用到Graph.as_default()的上下文管理器(context manager),它能够在这个上下文里面覆盖默认的图.要在某个graph里面定义量,要在with语句的范围里面定义\n",
    "#二.Session(tf.Session)\n",
    "#运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境\n",
    "#tensorflow中的计算以图数据流的方式表示，一个图包含一系列表示计算单元的操作对象，以及在图中流动的数据单元以tensor对象\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)#按axis=1求和降维，得(?,),*表示对应元素相乘\n",
    "        inference = tf.expand_dims(inference, axis=1) #(batch,1)为了下面和target统一格式计算loss\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #将梯度在target network和learner间传递的功能在distributed tensorflow中默认已经实现好了\n",
    "        #Between-graph的方式中，每个thread会拷贝一份Graph，计算后回传回主Graph。需要解决的主要是梯度累积的问题。\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference ) #在训练过程中增加了平方和loss 返回加权损失浮动Tensor.\n",
    "        loss = tf.reduce_mean(cost) #函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值\n",
    "    \n",
    "    \n",
    "    # 优化损失 \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr) #是一个寻找全局最优点的优化算法，引入了二次方梯度校正。 利用反向传播算法对权重和偏置项进行修正外，也在运行中不断修正学习率。\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    #minimize() = compute_gradients() + apply_gradients()拆分成计算梯度和应用梯度两个步骤\n",
    "    #tf.train.Optimizer.compute_gradients(loss, var_list=None, gate_gradients=1) 计算“var_list”中变量的“loss”梯度\n",
    "    #loss：包含要最小化的值的Tensor\n",
    "\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    #用 Adam 这个优化器来最小化 loss  应用梯度下降到变量上，用于更新变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "motivated-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceramic-insight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to D:\\something\\design\\code_new\\runs\\1619109289\n",
      "\n",
      "2021-04-23T00:34:54.490165: Epoch   0 Batch    0/3125   train_loss = 26.377 这是训练集\n",
      "2021-04-23T00:34:55.083751: Epoch   0 Batch   20/3125   train_loss = 6.943 这是训练集\n",
      "2021-04-23T00:34:55.691400: Epoch   0 Batch   40/3125   train_loss = 3.959 这是训练集\n",
      "2021-04-23T00:34:56.317991: Epoch   0 Batch   60/3125   train_loss = 3.254 这是训练集\n",
      "2021-04-23T00:34:56.889965: Epoch   0 Batch   80/3125   train_loss = 2.448 这是训练集\n",
      "2021-04-23T00:34:57.467918: Epoch   0 Batch  100/3125   train_loss = 2.148 这是训练集\n",
      "2021-04-23T00:34:58.045837: Epoch   0 Batch  120/3125   train_loss = 2.122 这是训练集\n",
      "2021-04-23T00:34:58.580296: Epoch   0 Batch  140/3125   train_loss = 1.820 这是训练集\n",
      "2021-04-23T00:34:59.197127: Epoch   0 Batch  160/3125   train_loss = 1.810 这是训练集\n",
      "2021-04-23T00:34:59.814466: Epoch   0 Batch  180/3125   train_loss = 1.456 这是训练集\n",
      "2021-04-23T00:35:00.424021: Epoch   0 Batch  200/3125   train_loss = 2.069 这是训练集\n",
      "2021-04-23T00:35:01.043347: Epoch   0 Batch  220/3125   train_loss = 1.612 这是训练集\n",
      "2021-04-23T00:35:01.683639: Epoch   0 Batch  240/3125   train_loss = 1.581 这是训练集\n",
      "2021-04-23T00:35:02.354151: Epoch   0 Batch  260/3125   train_loss = 1.502 这是训练集\n",
      "2021-04-23T00:35:03.028914: Epoch   0 Batch  280/3125   train_loss = 1.663 这是训练集\n",
      "2021-04-23T00:35:03.727808: Epoch   0 Batch  300/3125   train_loss = 1.528 这是训练集\n",
      "2021-04-23T00:35:04.460784: Epoch   0 Batch  320/3125   train_loss = 1.640 这是训练集\n",
      "2021-04-23T00:35:05.221599: Epoch   0 Batch  340/3125   train_loss = 1.455 这是训练集\n",
      "2021-04-23T00:35:05.965240: Epoch   0 Batch  360/3125   train_loss = 1.467 这是训练集\n",
      "2021-04-23T00:35:06.644535: Epoch   0 Batch  380/3125   train_loss = 1.300 这是训练集\n",
      "2021-04-23T00:35:07.342375: Epoch   0 Batch  400/3125   train_loss = 1.232 这是训练集\n",
      "2021-04-23T00:35:07.969118: Epoch   0 Batch  420/3125   train_loss = 1.401 这是训练集\n",
      "2021-04-23T00:35:08.540077: Epoch   0 Batch  440/3125   train_loss = 1.428 这是训练集\n",
      "2021-04-23T00:35:09.262668: Epoch   0 Batch  460/3125   train_loss = 1.476 这是训练集\n",
      "2021-04-23T00:35:09.914965: Epoch   0 Batch  480/3125   train_loss = 1.328 这是训练集\n",
      "2021-04-23T00:35:10.493212: Epoch   0 Batch  500/3125   train_loss = 1.110 这是训练集\n",
      "2021-04-23T00:35:11.069052: Epoch   0 Batch  520/3125   train_loss = 1.459 这是训练集\n",
      "2021-04-23T00:35:11.657581: Epoch   0 Batch  540/3125   train_loss = 1.256 这是训练集\n",
      "2021-04-23T00:35:12.295414: Epoch   0 Batch  560/3125   train_loss = 1.395 这是训练集\n",
      "2021-04-23T00:35:12.840999: Epoch   0 Batch  580/3125   train_loss = 1.389 这是训练集\n",
      "2021-04-23T00:35:13.432454: Epoch   0 Batch  600/3125   train_loss = 1.338 这是训练集\n",
      "2021-04-23T00:35:14.036114: Epoch   0 Batch  620/3125   train_loss = 1.454 这是训练集\n",
      "2021-04-23T00:35:14.590478: Epoch   0 Batch  640/3125   train_loss = 1.362 这是训练集\n",
      "2021-04-23T00:35:15.180534: Epoch   0 Batch  660/3125   train_loss = 1.392 这是训练集\n",
      "2021-04-23T00:35:15.783701: Epoch   0 Batch  680/3125   train_loss = 1.175 这是训练集\n",
      "2021-04-23T00:35:16.384486: Epoch   0 Batch  700/3125   train_loss = 1.365 这是训练集\n",
      "2021-04-23T00:35:16.963539: Epoch   0 Batch  720/3125   train_loss = 1.244 这是训练集\n",
      "2021-04-23T00:35:17.589606: Epoch   0 Batch  740/3125   train_loss = 1.331 这是训练集\n",
      "2021-04-23T00:35:18.224006: Epoch   0 Batch  760/3125   train_loss = 1.326 这是训练集\n",
      "2021-04-23T00:35:18.788141: Epoch   0 Batch  780/3125   train_loss = 1.481 这是训练集\n",
      "2021-04-23T00:35:19.373272: Epoch   0 Batch  800/3125   train_loss = 1.253 这是训练集\n",
      "2021-04-23T00:35:19.976439: Epoch   0 Batch  820/3125   train_loss = 1.288 这是训练集\n",
      "2021-04-23T00:35:20.563247: Epoch   0 Batch  840/3125   train_loss = 1.164 这是训练集\n",
      "2021-04-23T00:35:21.166806: Epoch   0 Batch  860/3125   train_loss = 1.254 这是训练集\n",
      "2021-04-23T00:35:21.778363: Epoch   0 Batch  880/3125   train_loss = 1.268 这是训练集\n",
      "2021-04-23T00:35:22.367867: Epoch   0 Batch  900/3125   train_loss = 1.272 这是训练集\n",
      "2021-04-23T00:35:22.932970: Epoch   0 Batch  920/3125   train_loss = 1.320 这是训练集\n",
      "2021-04-23T00:35:23.535677: Epoch   0 Batch  940/3125   train_loss = 1.345 这是训练集\n",
      "2021-04-23T00:35:24.178899: Epoch   0 Batch  960/3125   train_loss = 1.326 这是训练集\n",
      "2021-04-23T00:35:24.783041: Epoch   0 Batch  980/3125   train_loss = 1.334 这是训练集\n",
      "2021-04-23T00:35:25.387185: Epoch   0 Batch 1000/3125   train_loss = 1.300 这是训练集\n",
      "2021-04-23T00:35:26.005500: Epoch   0 Batch 1020/3125   train_loss = 1.361 这是训练集\n",
      "2021-04-23T00:35:26.608666: Epoch   0 Batch 1040/3125   train_loss = 1.269 这是训练集\n",
      "2021-04-23T00:35:27.198682: Epoch   0 Batch 1060/3125   train_loss = 1.442 这是训练集\n",
      "2021-04-23T00:35:27.876023: Epoch   0 Batch 1080/3125   train_loss = 1.178 这是训练集\n",
      "2021-04-23T00:35:28.449910: Epoch   0 Batch 1100/3125   train_loss = 1.316 这是训练集\n",
      "2021-04-23T00:35:29.031606: Epoch   0 Batch 1120/3125   train_loss = 1.309 这是训练集\n",
      "2021-04-23T00:35:29.663113: Epoch   0 Batch 1140/3125   train_loss = 1.352 这是训练集\n",
      "2021-04-23T00:35:30.265892: Epoch   0 Batch 1160/3125   train_loss = 1.333 这是训练集\n",
      "2021-04-23T00:35:30.899823: Epoch   0 Batch 1180/3125   train_loss = 1.297 这是训练集\n",
      "2021-04-23T00:35:31.532270: Epoch   0 Batch 1200/3125   train_loss = 1.278 这是训练集\n",
      "2021-04-23T00:35:32.149102: Epoch   0 Batch 1220/3125   train_loss = 1.203 这是训练集\n",
      "2021-04-23T00:35:32.717594: Epoch   0 Batch 1240/3125   train_loss = 1.088 这是训练集\n",
      "2021-04-23T00:35:33.321738: Epoch   0 Batch 1260/3125   train_loss = 1.223 这是训练集\n",
      "2021-04-23T00:35:33.921550: Epoch   0 Batch 1280/3125   train_loss = 1.210 这是训练集\n",
      "2021-04-23T00:35:34.502828: Epoch   0 Batch 1300/3125   train_loss = 1.288 这是训练集\n",
      "2021-04-23T00:35:35.144138: Epoch   0 Batch 1320/3125   train_loss = 1.195 这是训练集\n",
      "2021-04-23T00:35:35.807817: Epoch   0 Batch 1340/3125   train_loss = 1.057 这是训练集\n",
      "2021-04-23T00:35:36.407625: Epoch   0 Batch 1360/3125   train_loss = 1.201 这是训练集\n",
      "2021-04-23T00:35:37.000568: Epoch   0 Batch 1380/3125   train_loss = 1.109 这是训练集\n",
      "2021-04-23T00:35:37.634499: Epoch   0 Batch 1400/3125   train_loss = 1.300 这是训练集\n",
      "2021-04-23T00:35:38.245474: Epoch   0 Batch 1420/3125   train_loss = 1.299 这是训练集\n",
      "2021-04-23T00:35:38.814144: Epoch   0 Batch 1440/3125   train_loss = 1.157 这是训练集\n",
      "2021-04-23T00:35:39.415150: Epoch   0 Batch 1460/3125   train_loss = 1.315 这是训练集\n",
      "2021-04-23T00:35:40.029432: Epoch   0 Batch 1480/3125   train_loss = 1.314 这是训练集\n",
      "2021-04-23T00:35:40.582823: Epoch   0 Batch 1500/3125   train_loss = 1.336 这是训练集\n",
      "2021-04-23T00:35:41.235811: Epoch   0 Batch 1520/3125   train_loss = 1.302 这是训练集\n",
      "2021-04-23T00:35:41.873119: Epoch   0 Batch 1540/3125   train_loss = 1.320 这是训练集\n",
      "2021-04-23T00:35:42.444845: Epoch   0 Batch 1560/3125   train_loss = 1.194 这是训练集\n",
      "2021-04-23T00:35:43.040204: Epoch   0 Batch 1580/3125   train_loss = 1.281 这是训练集\n",
      "2021-04-23T00:35:43.647783: Epoch   0 Batch 1600/3125   train_loss = 1.233 这是训练集\n",
      "2021-04-23T00:35:44.261379: Epoch   0 Batch 1620/3125   train_loss = 1.222 这是训练集\n",
      "2021-04-23T00:35:44.823497: Epoch   0 Batch 1640/3125   train_loss = 1.319 这是训练集\n",
      "2021-04-23T00:35:45.594536: Epoch   0 Batch 1660/3125   train_loss = 1.310 这是训练集\n",
      "2021-04-23T00:35:46.268017: Epoch   0 Batch 1680/3125   train_loss = 1.192 这是训练集\n",
      "2021-04-23T00:35:46.912088: Epoch   0 Batch 1700/3125   train_loss = 1.125 这是训练集\n",
      "2021-04-23T00:35:47.556756: Epoch   0 Batch 1720/3125   train_loss = 1.191 这是训练集\n",
      "2021-04-23T00:35:48.198963: Epoch   0 Batch 1740/3125   train_loss = 1.215 这是训练集\n",
      "2021-04-23T00:35:48.756770: Epoch   0 Batch 1760/3125   train_loss = 1.279 这是训练集\n",
      "2021-04-23T00:35:49.344866: Epoch   0 Batch 1780/3125   train_loss = 1.175 这是训练集\n",
      "2021-04-23T00:35:49.963183: Epoch   0 Batch 1800/3125   train_loss = 1.214 这是训练集\n",
      "2021-04-23T00:35:50.577086: Epoch   0 Batch 1820/3125   train_loss = 1.219 这是训练集\n",
      "2021-04-23T00:35:51.363366: Epoch   0 Batch 1840/3125   train_loss = 1.303 这是训练集\n",
      "2021-04-23T00:35:52.079036: Epoch   0 Batch 1860/3125   train_loss = 1.295 这是训练集\n",
      "2021-04-23T00:35:52.656394: Epoch   0 Batch 1880/3125   train_loss = 1.292 这是训练集\n",
      "2021-04-23T00:35:53.258586: Epoch   0 Batch 1900/3125   train_loss = 1.070 这是训练集\n",
      "2021-04-23T00:35:53.856468: Epoch   0 Batch 1920/3125   train_loss = 1.166 这是训练集\n",
      "2021-04-23T00:35:54.434250: Epoch   0 Batch 1940/3125   train_loss = 1.173 这是训练集\n",
      "2021-04-23T00:35:55.012720: Epoch   0 Batch 1960/3125   train_loss = 1.127 这是训练集\n",
      "2021-04-23T00:35:55.612960: Epoch   0 Batch 1980/3125   train_loss = 1.122 这是训练集\n",
      "2021-04-23T00:35:56.205431: Epoch   0 Batch 2000/3125   train_loss = 1.468 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:35:56.777911: Epoch   0 Batch 2020/3125   train_loss = 1.261 这是训练集\n",
      "2021-04-23T00:35:57.430436: Epoch   0 Batch 2040/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:35:58.080998: Epoch   0 Batch 2060/3125   train_loss = 1.021 这是训练集\n",
      "2021-04-23T00:35:58.631461: Epoch   0 Batch 2080/3125   train_loss = 1.332 这是训练集\n",
      "2021-04-23T00:35:59.420496: Epoch   0 Batch 2100/3125   train_loss = 1.144 这是训练集\n",
      "2021-04-23T00:36:00.209820: Epoch   0 Batch 2120/3125   train_loss = 1.052 这是训练集\n",
      "2021-04-23T00:36:01.069336: Epoch   0 Batch 2140/3125   train_loss = 1.170 这是训练集\n",
      "2021-04-23T00:36:01.898101: Epoch   0 Batch 2160/3125   train_loss = 1.189 这是训练集\n",
      "2021-04-23T00:36:02.735549: Epoch   0 Batch 2180/3125   train_loss = 1.176 这是训练集\n",
      "2021-04-23T00:36:03.389467: Epoch   0 Batch 2200/3125   train_loss = 1.111 这是训练集\n",
      "2021-04-23T00:36:03.995609: Epoch   0 Batch 2220/3125   train_loss = 1.157 这是训练集\n",
      "2021-04-23T00:36:04.713521: Epoch   0 Batch 2240/3125   train_loss = 1.013 这是训练集\n",
      "2021-04-23T00:36:05.429477: Epoch   0 Batch 2260/3125   train_loss = 1.159 这是训练集\n",
      "2021-04-23T00:36:06.126340: Epoch   0 Batch 2280/3125   train_loss = 1.249 这是训练集\n",
      "2021-04-23T00:36:06.697867: Epoch   0 Batch 2300/3125   train_loss = 1.180 这是训练集\n",
      "2021-04-23T00:36:07.344999: Epoch   0 Batch 2320/3125   train_loss = 1.304 这是训练集\n",
      "2021-04-23T00:36:07.958902: Epoch   0 Batch 2340/3125   train_loss = 1.289 这是训练集\n",
      "2021-04-23T00:36:08.531850: Epoch   0 Batch 2360/3125   train_loss = 1.193 这是训练集\n",
      "2021-04-23T00:36:09.125464: Epoch   0 Batch 2380/3125   train_loss = 1.212 这是训练集\n",
      "2021-04-23T00:36:09.731559: Epoch   0 Batch 2400/3125   train_loss = 1.267 这是训练集\n",
      "2021-04-23T00:36:10.356198: Epoch   0 Batch 2420/3125   train_loss = 1.146 这是训练集\n",
      "2021-04-23T00:36:10.927851: Epoch   0 Batch 2440/3125   train_loss = 1.270 这是训练集\n",
      "2021-04-23T00:36:11.565264: Epoch   0 Batch 2460/3125   train_loss = 1.181 这是训练集\n",
      "2021-04-23T00:36:12.162108: Epoch   0 Batch 2480/3125   train_loss = 1.220 这是训练集\n",
      "2021-04-23T00:36:12.736972: Epoch   0 Batch 2500/3125   train_loss = 1.195 这是训练集\n",
      "2021-04-23T00:36:13.319277: Epoch   0 Batch 2520/3125   train_loss = 1.144 这是训练集\n",
      "2021-04-23T00:36:13.902544: Epoch   0 Batch 2540/3125   train_loss = 1.105 这是训练集\n",
      "2021-04-23T00:36:14.461791: Epoch   0 Batch 2560/3125   train_loss = 0.990 这是训练集\n",
      "2021-04-23T00:36:15.043994: Epoch   0 Batch 2580/3125   train_loss = 1.145 这是训练集\n",
      "2021-04-23T00:36:15.628669: Epoch   0 Batch 2600/3125   train_loss = 1.154 这是训练集\n",
      "2021-04-23T00:36:16.219940: Epoch   0 Batch 2620/3125   train_loss = 1.110 这是训练集\n",
      "2021-04-23T00:36:16.775283: Epoch   0 Batch 2640/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:36:17.465869: Epoch   0 Batch 2660/3125   train_loss = 1.183 这是训练集\n",
      "2021-04-23T00:36:18.115762: Epoch   0 Batch 2680/3125   train_loss = 1.052 这是训练集\n",
      "2021-04-23T00:36:18.780417: Epoch   0 Batch 2700/3125   train_loss = 1.258 这是训练集\n",
      "2021-04-23T00:36:19.429963: Epoch   0 Batch 2720/3125   train_loss = 1.149 这是训练集\n",
      "2021-04-23T00:36:20.018361: Epoch   0 Batch 2740/3125   train_loss = 1.195 这是训练集\n",
      "2021-04-23T00:36:20.574211: Epoch   0 Batch 2760/3125   train_loss = 1.199 这是训练集\n",
      "2021-04-23T00:36:21.161762: Epoch   0 Batch 2780/3125   train_loss = 1.117 这是训练集\n",
      "2021-04-23T00:36:21.754704: Epoch   0 Batch 2800/3125   train_loss = 1.366 这是训练集\n",
      "2021-04-23T00:36:22.334512: Epoch   0 Batch 2820/3125   train_loss = 1.352 这是训练集\n",
      "2021-04-23T00:36:22.912811: Epoch   0 Batch 2840/3125   train_loss = 1.159 这是训练集\n",
      "2021-04-23T00:36:23.517932: Epoch   0 Batch 2860/3125   train_loss = 1.126 这是训练集\n",
      "2021-04-23T00:36:24.149479: Epoch   0 Batch 2880/3125   train_loss = 1.147 这是训练集\n",
      "2021-04-23T00:36:24.728435: Epoch   0 Batch 2900/3125   train_loss = 1.130 这是训练集\n",
      "2021-04-23T00:36:25.344291: Epoch   0 Batch 2920/3125   train_loss = 1.198 这是训练集\n",
      "2021-04-23T00:36:25.950387: Epoch   0 Batch 2940/3125   train_loss = 1.131 这是训练集\n",
      "2021-04-23T00:36:26.500854: Epoch   0 Batch 2960/3125   train_loss = 1.234 这是训练集\n",
      "2021-04-23T00:36:27.088915: Epoch   0 Batch 2980/3125   train_loss = 1.138 这是训练集\n",
      "2021-04-23T00:36:27.686228: Epoch   0 Batch 3000/3125   train_loss = 1.229 这是训练集\n",
      "2021-04-23T00:36:28.271865: Epoch   0 Batch 3020/3125   train_loss = 1.229 这是训练集\n",
      "2021-04-23T00:36:28.831692: Epoch   0 Batch 3040/3125   train_loss = 1.148 这是训练集\n",
      "2021-04-23T00:36:29.453483: Epoch   0 Batch 3060/3125   train_loss = 1.216 这是训练集\n",
      "2021-04-23T00:36:30.106426: Epoch   0 Batch 3080/3125   train_loss = 1.264 这是训练集\n",
      "2021-04-23T00:36:30.669583: Epoch   0 Batch 3100/3125   train_loss = 1.264 这是训练集\n",
      "2021-04-23T00:36:31.286402: Epoch   0 Batch 3120/3125   train_loss = 1.051 这是训练集\n",
      "2021-04-23T00:36:31.487379: Epoch   0 Batch    0/781   test_loss = 0.954 这是测试集 \n",
      "2021-04-23T00:36:31.693353: Epoch   0 Batch   20/781   test_loss = 1.150 这是测试集 \n",
      "2021-04-23T00:36:31.890548: Epoch   0 Batch   40/781   test_loss = 1.055 这是测试集 \n",
      "2021-04-23T00:36:32.090159: Epoch   0 Batch   60/781   test_loss = 1.254 这是测试集 \n",
      "2021-04-23T00:36:32.275599: Epoch   0 Batch   80/781   test_loss = 1.385 这是测试集 \n",
      "2021-04-23T00:36:32.449327: Epoch   0 Batch  100/781   test_loss = 1.335 这是测试集 \n",
      "2021-04-23T00:36:32.641599: Epoch   0 Batch  120/781   test_loss = 1.196 这是测试集 \n",
      "2021-04-23T00:36:32.842654: Epoch   0 Batch  140/781   test_loss = 1.176 这是测试集 \n",
      "2021-04-23T00:36:33.038830: Epoch   0 Batch  160/781   test_loss = 1.351 这是测试集 \n",
      "2021-04-23T00:36:33.235982: Epoch   0 Batch  180/781   test_loss = 1.201 这是测试集 \n",
      "2021-04-23T00:36:33.430715: Epoch   0 Batch  200/781   test_loss = 1.146 这是测试集 \n",
      "2021-04-23T00:36:33.631771: Epoch   0 Batch  220/781   test_loss = 0.954 这是测试集 \n",
      "2021-04-23T00:36:33.830878: Epoch   0 Batch  240/781   test_loss = 1.152 这是测试集 \n",
      "2021-04-23T00:36:34.031425: Epoch   0 Batch  260/781   test_loss = 1.171 这是测试集 \n",
      "2021-04-23T00:36:34.225569: Epoch   0 Batch  280/781   test_loss = 1.402 这是测试集 \n",
      "2021-04-23T00:36:34.407105: Epoch   0 Batch  300/781   test_loss = 1.197 这是测试集 \n",
      "2021-04-23T00:36:34.600353: Epoch   0 Batch  320/781   test_loss = 1.267 这是测试集 \n",
      "2021-04-23T00:36:34.791648: Epoch   0 Batch  340/781   test_loss = 0.907 这是测试集 \n",
      "2021-04-23T00:36:34.983920: Epoch   0 Batch  360/781   test_loss = 1.355 这是测试集 \n",
      "2021-04-23T00:36:35.182048: Epoch   0 Batch  380/781   test_loss = 1.138 这是测试集 \n",
      "2021-04-23T00:36:35.381529: Epoch   0 Batch  400/781   test_loss = 1.032 这是测试集 \n",
      "2021-04-23T00:36:35.590201: Epoch   0 Batch  420/781   test_loss = 1.046 这是测试集 \n",
      "2021-04-23T00:36:35.797897: Epoch   0 Batch  440/781   test_loss = 1.242 这是测试集 \n",
      "2021-04-23T00:36:35.991264: Epoch   0 Batch  460/781   test_loss = 1.152 这是测试集 \n",
      "2021-04-23T00:36:36.189900: Epoch   0 Batch  480/781   test_loss = 1.111 这是测试集 \n",
      "2021-04-23T00:36:36.369483: Epoch   0 Batch  500/781   test_loss = 0.994 这是测试集 \n",
      "2021-04-23T00:36:36.546140: Epoch   0 Batch  520/781   test_loss = 1.205 这是测试集 \n",
      "2021-04-23T00:36:36.746219: Epoch   0 Batch  540/781   test_loss = 1.114 这是测试集 \n",
      "2021-04-23T00:36:36.955083: Epoch   0 Batch  560/781   test_loss = 1.242 这是测试集 \n",
      "2021-04-23T00:36:37.160043: Epoch   0 Batch  580/781   test_loss = 1.131 这是测试集 \n",
      "2021-04-23T00:36:37.374764: Epoch   0 Batch  600/781   test_loss = 1.194 这是测试集 \n",
      "2021-04-23T00:36:37.622709: Epoch   0 Batch  620/781   test_loss = 1.185 这是测试集 \n",
      "2021-04-23T00:36:37.848403: Epoch   0 Batch  640/781   test_loss = 1.257 这是测试集 \n",
      "2021-04-23T00:36:38.047508: Epoch   0 Batch  660/781   test_loss = 1.192 这是测试集 \n",
      "2021-04-23T00:36:38.248949: Epoch   0 Batch  680/781   test_loss = 1.415 这是测试集 \n",
      "2021-04-23T00:36:38.419749: Epoch   0 Batch  700/781   test_loss = 1.139 这是测试集 \n",
      "2021-04-23T00:36:38.608118: Epoch   0 Batch  720/781   test_loss = 1.263 这是测试集 \n",
      "2021-04-23T00:36:38.804292: Epoch   0 Batch  740/781   test_loss = 1.191 这是测试集 \n",
      "2021-04-23T00:36:38.998517: Epoch   0 Batch  760/781   test_loss = 1.127 这是测试集 \n",
      "2021-04-23T00:36:39.204452: Epoch   0 Batch  780/781   test_loss = 1.156 这是测试集 \n",
      "2021-04-23T00:36:40.696388: Epoch   1 Batch   15/3125   train_loss = 1.206 这是训练集\n",
      "2021-04-23T00:36:41.300533: Epoch   1 Batch   35/3125   train_loss = 1.141 这是训练集\n",
      "2021-04-23T00:36:41.954451: Epoch   1 Batch   55/3125   train_loss = 1.246 这是训练集\n",
      "2021-04-23T00:36:42.509870: Epoch   1 Batch   75/3125   train_loss = 1.148 这是训练集\n",
      "2021-04-23T00:36:43.134959: Epoch   1 Batch   95/3125   train_loss = 1.007 这是训练集\n",
      "2021-04-23T00:36:43.813278: Epoch   1 Batch  115/3125   train_loss = 1.257 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:36:44.387053: Epoch   1 Batch  135/3125   train_loss = 1.028 这是训练集\n",
      "2021-04-23T00:36:44.980623: Epoch   1 Batch  155/3125   train_loss = 1.115 这是训练集\n",
      "2021-04-23T00:36:45.592613: Epoch   1 Batch  175/3125   train_loss = 1.104 这是训练集\n",
      "2021-04-23T00:36:46.237749: Epoch   1 Batch  195/3125   train_loss = 1.188 这是训练集\n",
      "2021-04-23T00:36:46.828964: Epoch   1 Batch  215/3125   train_loss = 1.118 这是训练集\n",
      "2021-04-23T00:36:47.436543: Epoch   1 Batch  235/3125   train_loss = 1.075 这是训练集\n",
      "2021-04-23T00:36:48.034831: Epoch   1 Batch  255/3125   train_loss = 1.250 这是训练集\n",
      "2021-04-23T00:36:48.605790: Epoch   1 Batch  275/3125   train_loss = 1.034 这是训练集\n",
      "2021-04-23T00:36:49.306445: Epoch   1 Batch  295/3125   train_loss = 1.124 这是训练集\n",
      "2021-04-23T00:36:49.951619: Epoch   1 Batch  315/3125   train_loss = 1.107 这是训练集\n",
      "2021-04-23T00:36:50.668001: Epoch   1 Batch  335/3125   train_loss = 0.996 这是训练集\n",
      "2021-04-23T00:36:51.302608: Epoch   1 Batch  355/3125   train_loss = 1.132 这是训练集\n",
      "2021-04-23T00:36:51.916308: Epoch   1 Batch  375/3125   train_loss = 1.210 这是训练集\n",
      "2021-04-23T00:36:52.474578: Epoch   1 Batch  395/3125   train_loss = 1.042 这是训练集\n",
      "2021-04-23T00:36:53.130450: Epoch   1 Batch  415/3125   train_loss = 1.321 这是训练集\n",
      "2021-04-23T00:36:53.731943: Epoch   1 Batch  435/3125   train_loss = 1.210 这是训练集\n",
      "2021-04-23T00:36:54.321238: Epoch   1 Batch  455/3125   train_loss = 1.089 这是训练集\n",
      "2021-04-23T00:36:54.900045: Epoch   1 Batch  475/3125   train_loss = 1.193 这是训练集\n",
      "2021-04-23T00:36:55.502237: Epoch   1 Batch  495/3125   train_loss = 1.079 这是训练集\n",
      "2021-04-23T00:36:56.100011: Epoch   1 Batch  515/3125   train_loss = 1.199 这是训练集\n",
      "2021-04-23T00:36:56.648528: Epoch   1 Batch  535/3125   train_loss = 1.198 这是训练集\n",
      "2021-04-23T00:36:57.245394: Epoch   1 Batch  555/3125   train_loss = 1.288 这是训练集\n",
      "2021-04-23T00:36:57.845164: Epoch   1 Batch  575/3125   train_loss = 1.112 这是训练集\n",
      "2021-04-23T00:36:58.436620: Epoch   1 Batch  595/3125   train_loss = 1.245 这是训练集\n",
      "2021-04-23T00:36:59.047645: Epoch   1 Batch  615/3125   train_loss = 1.036 这是训练集\n",
      "2021-04-23T00:36:59.733556: Epoch   1 Batch  635/3125   train_loss = 1.146 这是训练集\n",
      "2021-04-23T00:37:00.346522: Epoch   1 Batch  655/3125   train_loss = 1.067 这是训练集\n",
      "2021-04-23T00:37:00.911626: Epoch   1 Batch  675/3125   train_loss = 0.888 这是训练集\n",
      "2021-04-23T00:37:01.527013: Epoch   1 Batch  695/3125   train_loss = 1.113 这是训练集\n",
      "2021-04-23T00:37:02.151021: Epoch   1 Batch  715/3125   train_loss = 1.136 这是训练集\n",
      "2021-04-23T00:37:02.696134: Epoch   1 Batch  735/3125   train_loss = 0.997 这是训练集\n",
      "2021-04-23T00:37:03.284662: Epoch   1 Batch  755/3125   train_loss = 1.220 这是训练集\n",
      "2021-04-23T00:37:03.866357: Epoch   1 Batch  775/3125   train_loss = 1.010 这是训练集\n",
      "2021-04-23T00:37:04.433412: Epoch   1 Batch  795/3125   train_loss = 1.237 这是训练集\n",
      "2021-04-23T00:37:05.071327: Epoch   1 Batch  815/3125   train_loss = 1.157 这是训练集\n",
      "2021-04-23T00:37:05.688666: Epoch   1 Batch  835/3125   train_loss = 1.105 这是训练集\n",
      "2021-04-23T00:37:06.319162: Epoch   1 Batch  855/3125   train_loss = 1.254 这是训练集\n",
      "2021-04-23T00:37:06.909778: Epoch   1 Batch  875/3125   train_loss = 1.190 这是训练集\n",
      "2021-04-23T00:37:07.510530: Epoch   1 Batch  895/3125   train_loss = 1.130 这是训练集\n",
      "2021-04-23T00:37:08.124433: Epoch   1 Batch  915/3125   train_loss = 1.134 这是训练集\n",
      "2021-04-23T00:37:08.687585: Epoch   1 Batch  935/3125   train_loss = 1.246 这是训练集\n",
      "2021-04-23T00:37:09.292773: Epoch   1 Batch  955/3125   train_loss = 1.245 这是训练集\n",
      "2021-04-23T00:37:09.949659: Epoch   1 Batch  975/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:37:10.529401: Epoch   1 Batch  995/3125   train_loss = 0.996 这是训练集\n",
      "2021-04-23T00:37:11.118438: Epoch   1 Batch 1015/3125   train_loss = 1.099 这是训练集\n",
      "2021-04-23T00:37:11.724771: Epoch   1 Batch 1035/3125   train_loss = 1.090 这是训练集\n",
      "2021-04-23T00:37:12.321107: Epoch   1 Batch 1055/3125   train_loss = 1.111 这是训练集\n",
      "2021-04-23T00:37:12.879379: Epoch   1 Batch 1075/3125   train_loss = 1.092 这是训练集\n",
      "2021-04-23T00:37:13.479658: Epoch   1 Batch 1095/3125   train_loss = 0.990 这是训练集\n",
      "2021-04-23T00:37:14.075065: Epoch   1 Batch 1115/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:37:14.645047: Epoch   1 Batch 1135/3125   train_loss = 1.045 这是训练集\n",
      "2021-04-23T00:37:15.271169: Epoch   1 Batch 1155/3125   train_loss = 1.154 这是训练集\n",
      "2021-04-23T00:37:15.970954: Epoch   1 Batch 1175/3125   train_loss = 1.093 这是训练集\n",
      "2021-04-23T00:37:16.523880: Epoch   1 Batch 1195/3125   train_loss = 1.242 这是训练集\n",
      "2021-04-23T00:37:17.167064: Epoch   1 Batch 1215/3125   train_loss = 1.002 这是训练集\n",
      "2021-04-23T00:37:17.781738: Epoch   1 Batch 1235/3125   train_loss = 1.106 这是训练集\n",
      "2021-04-23T00:37:18.357687: Epoch   1 Batch 1255/3125   train_loss = 0.970 这是训练集\n",
      "2021-04-23T00:37:18.943286: Epoch   1 Batch 1275/3125   train_loss = 1.002 这是训练集\n",
      "2021-04-23T00:37:19.665057: Epoch   1 Batch 1295/3125   train_loss = 1.083 这是训练集\n",
      "2021-04-23T00:37:20.299773: Epoch   1 Batch 1315/3125   train_loss = 1.166 这是训练集\n",
      "2021-04-23T00:37:20.881203: Epoch   1 Batch 1335/3125   train_loss = 1.149 这是训练集\n",
      "2021-04-23T00:37:21.491202: Epoch   1 Batch 1355/3125   train_loss = 1.092 这是训练集\n",
      "2021-04-23T00:37:22.168469: Epoch   1 Batch 1375/3125   train_loss = 1.174 这是训练集\n",
      "2021-04-23T00:37:22.784051: Epoch   1 Batch 1395/3125   train_loss = 1.107 这是训练集\n",
      "2021-04-23T00:37:23.387217: Epoch   1 Batch 1415/3125   train_loss = 1.089 这是训练集\n",
      "2021-04-23T00:37:24.009905: Epoch   1 Batch 1435/3125   train_loss = 1.277 这是训练集\n",
      "2021-04-23T00:37:24.572634: Epoch   1 Batch 1455/3125   train_loss = 1.221 这是训练集\n",
      "2021-04-23T00:37:25.179365: Epoch   1 Batch 1475/3125   train_loss = 1.129 这是训练集\n",
      "2021-04-23T00:37:25.795220: Epoch   1 Batch 1495/3125   train_loss = 1.017 这是训练集\n",
      "2021-04-23T00:37:26.387182: Epoch   1 Batch 1515/3125   train_loss = 0.953 这是训练集\n",
      "2021-04-23T00:37:26.959196: Epoch   1 Batch 1535/3125   train_loss = 0.936 这是训练集\n",
      "2021-04-23T00:37:27.553664: Epoch   1 Batch 1555/3125   train_loss = 1.046 这是训练集\n",
      "2021-04-23T00:37:28.144225: Epoch   1 Batch 1575/3125   train_loss = 1.023 这是训练集\n",
      "2021-04-23T00:37:28.733729: Epoch   1 Batch 1595/3125   train_loss = 1.154 这是训练集\n",
      "2021-04-23T00:37:29.326197: Epoch   1 Batch 1615/3125   train_loss = 1.029 这是训练集\n",
      "2021-04-23T00:37:29.937150: Epoch   1 Batch 1635/3125   train_loss = 1.149 这是训练集\n",
      "2021-04-23T00:37:30.495930: Epoch   1 Batch 1655/3125   train_loss = 1.141 这是训练集\n",
      "2021-04-23T00:37:31.082505: Epoch   1 Batch 1675/3125   train_loss = 1.014 这是训练集\n",
      "2021-04-23T00:37:31.674507: Epoch   1 Batch 1695/3125   train_loss = 1.059 这是训练集\n",
      "2021-04-23T00:37:32.258937: Epoch   1 Batch 1715/3125   train_loss = 0.948 这是训练集\n",
      "2021-04-23T00:37:32.816742: Epoch   1 Batch 1735/3125   train_loss = 1.164 这是训练集\n",
      "2021-04-23T00:37:33.445285: Epoch   1 Batch 1755/3125   train_loss = 1.014 这是训练集\n",
      "2021-04-23T00:37:34.101400: Epoch   1 Batch 1775/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:37:34.680931: Epoch   1 Batch 1795/3125   train_loss = 1.119 这是训练集\n",
      "2021-04-23T00:37:35.344003: Epoch   1 Batch 1815/3125   train_loss = 1.016 这是训练集\n",
      "2021-04-23T00:37:36.043324: Epoch   1 Batch 1835/3125   train_loss = 1.117 这是训练集\n",
      "2021-04-23T00:37:36.624796: Epoch   1 Batch 1855/3125   train_loss = 0.980 这是训练集\n",
      "2021-04-23T00:37:37.298723: Epoch   1 Batch 1875/3125   train_loss = 1.096 这是训练集\n",
      "2021-04-23T00:37:37.904856: Epoch   1 Batch 1895/3125   train_loss = 0.962 这是训练集\n",
      "2021-04-23T00:37:38.461176: Epoch   1 Batch 1915/3125   train_loss = 0.950 这是训练集\n",
      "2021-04-23T00:37:39.048260: Epoch   1 Batch 1935/3125   train_loss = 1.058 这是训练集\n",
      "2021-04-23T00:37:39.625156: Epoch   1 Batch 1955/3125   train_loss = 1.022 这是训练集\n",
      "2021-04-23T00:37:40.222974: Epoch   1 Batch 1975/3125   train_loss = 1.056 这是训练集\n",
      "2021-04-23T00:37:40.835939: Epoch   1 Batch 1995/3125   train_loss = 1.209 这是训练集\n",
      "2021-04-23T00:37:41.428371: Epoch   1 Batch 2015/3125   train_loss = 1.120 这是训练集\n",
      "2021-04-23T00:37:42.058501: Epoch   1 Batch 2035/3125   train_loss = 1.129 这是训练集\n",
      "2021-04-23T00:37:42.640175: Epoch   1 Batch 2055/3125   train_loss = 1.035 这是训练集\n",
      "2021-04-23T00:37:43.902649: Epoch   1 Batch 2075/3125   train_loss = 1.151 这是训练集\n",
      "2021-04-23T00:37:44.474412: Epoch   1 Batch 2095/3125   train_loss = 1.042 这是训练集\n",
      "2021-04-23T00:37:45.082850: Epoch   1 Batch 2115/3125   train_loss = 1.162 这是训练集\n",
      "2021-04-23T00:37:45.700657: Epoch   1 Batch 2135/3125   train_loss = 0.990 这是训练集\n",
      "2021-04-23T00:37:46.294670: Epoch   1 Batch 2155/3125   train_loss = 1.041 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:37:46.890541: Epoch   1 Batch 2175/3125   train_loss = 1.041 这是训练集\n",
      "2021-04-23T00:37:47.537672: Epoch   1 Batch 2195/3125   train_loss = 1.147 这是训练集\n",
      "2021-04-23T00:37:48.202833: Epoch   1 Batch 2215/3125   train_loss = 1.006 这是训练集\n",
      "2021-04-23T00:37:48.757659: Epoch   1 Batch 2235/3125   train_loss = 1.223 这是训练集\n",
      "2021-04-23T00:37:49.370587: Epoch   1 Batch 2255/3125   train_loss = 1.111 这是训练集\n",
      "2021-04-23T00:37:49.991869: Epoch   1 Batch 2275/3125   train_loss = 0.873 这是训练集\n",
      "2021-04-23T00:37:50.591475: Epoch   1 Batch 2295/3125   train_loss = 1.254 这是训练集\n",
      "2021-04-23T00:37:51.181994: Epoch   1 Batch 2315/3125   train_loss = 1.195 这是训练集\n",
      "2021-04-23T00:37:51.791015: Epoch   1 Batch 2335/3125   train_loss = 1.070 这是训练集\n",
      "2021-04-23T00:37:52.412765: Epoch   1 Batch 2355/3125   train_loss = 1.096 这是训练集\n",
      "2021-04-23T00:37:53.041532: Epoch   1 Batch 2375/3125   train_loss = 1.247 这是训练集\n",
      "2021-04-23T00:37:53.673980: Epoch   1 Batch 2395/3125   train_loss = 1.090 这是训练集\n",
      "2021-04-23T00:37:54.289370: Epoch   1 Batch 2415/3125   train_loss = 1.089 这是训练集\n",
      "2021-04-23T00:37:54.850102: Epoch   1 Batch 2435/3125   train_loss = 1.091 这是训练集\n",
      "2021-04-23T00:37:55.456329: Epoch   1 Batch 2455/3125   train_loss = 1.106 这是训练集\n",
      "2021-04-23T00:37:56.056568: Epoch   1 Batch 2475/3125   train_loss = 0.964 这是训练集\n",
      "2021-04-23T00:37:56.603128: Epoch   1 Batch 2495/3125   train_loss = 0.996 这是训练集\n",
      "2021-04-23T00:37:57.232648: Epoch   1 Batch 2515/3125   train_loss = 1.055 这是训练集\n",
      "2021-04-23T00:37:57.826132: Epoch   1 Batch 2535/3125   train_loss = 1.108 这是训练集\n",
      "2021-04-23T00:37:58.400995: Epoch   1 Batch 2555/3125   train_loss = 0.946 这是训练集\n",
      "2021-04-23T00:37:59.004163: Epoch   1 Batch 2575/3125   train_loss = 0.971 这是训练集\n",
      "2021-04-23T00:37:59.630829: Epoch   1 Batch 2595/3125   train_loss = 1.062 这是训练集\n",
      "2021-04-23T00:38:00.390664: Epoch   1 Batch 2615/3125   train_loss = 1.170 这是训练集\n",
      "2021-04-23T00:38:01.008998: Epoch   1 Batch 2635/3125   train_loss = 0.972 这是训练集\n",
      "2021-04-23T00:38:01.728516: Epoch   1 Batch 2655/3125   train_loss = 0.994 这是训练集\n",
      "2021-04-23T00:38:02.360450: Epoch   1 Batch 2675/3125   train_loss = 1.015 这是训练集\n",
      "2021-04-23T00:38:02.975329: Epoch   1 Batch 2695/3125   train_loss = 1.067 这是训练集\n",
      "2021-04-23T00:38:03.655133: Epoch   1 Batch 2715/3125   train_loss = 0.997 这是训练集\n",
      "2021-04-23T00:38:04.422816: Epoch   1 Batch 2735/3125   train_loss = 0.916 这是训练集\n",
      "2021-04-23T00:38:05.056752: Epoch   1 Batch 2755/3125   train_loss = 1.124 这是训练集\n",
      "2021-04-23T00:38:05.669583: Epoch   1 Batch 2775/3125   train_loss = 1.106 这是训练集\n",
      "2021-04-23T00:38:06.279152: Epoch   1 Batch 2795/3125   train_loss = 1.042 这是训练集\n",
      "2021-04-23T00:38:06.901840: Epoch   1 Batch 2815/3125   train_loss = 1.042 这是训练集\n",
      "2021-04-23T00:38:07.520158: Epoch   1 Batch 2835/3125   train_loss = 1.025 这是训练集\n",
      "2021-04-23T00:38:08.103611: Epoch   1 Batch 2855/3125   train_loss = 1.114 这是训练集\n",
      "2021-04-23T00:38:08.693115: Epoch   1 Batch 2875/3125   train_loss = 1.009 这是训练集\n",
      "2021-04-23T00:38:09.291277: Epoch   1 Batch 2895/3125   train_loss = 1.083 这是训练集\n",
      "2021-04-23T00:38:09.902766: Epoch   1 Batch 2915/3125   train_loss = 0.997 这是训练集\n",
      "2021-04-23T00:38:10.480355: Epoch   1 Batch 2935/3125   train_loss = 1.134 这是训练集\n",
      "2021-04-23T00:38:11.075714: Epoch   1 Batch 2955/3125   train_loss = 1.155 这是训练集\n",
      "2021-04-23T00:38:11.704280: Epoch   1 Batch 2975/3125   train_loss = 0.995 这是训练集\n",
      "2021-04-23T00:38:12.330122: Epoch   1 Batch 2995/3125   train_loss = 1.006 这是训练集\n",
      "2021-04-23T00:38:12.936762: Epoch   1 Batch 3015/3125   train_loss = 0.996 这是训练集\n",
      "2021-04-23T00:38:13.525288: Epoch   1 Batch 3035/3125   train_loss = 1.118 这是训练集\n",
      "2021-04-23T00:38:14.120327: Epoch   1 Batch 3055/3125   train_loss = 1.121 这是训练集\n",
      "2021-04-23T00:38:14.684458: Epoch   1 Batch 3075/3125   train_loss = 0.985 这是训练集\n",
      "2021-04-23T00:38:15.290553: Epoch   1 Batch 3095/3125   train_loss = 1.018 这是训练集\n",
      "2021-04-23T00:38:15.899826: Epoch   1 Batch 3115/3125   train_loss = 0.864 这是训练集\n",
      "2021-04-23T00:38:16.584417: Epoch   1 Batch   19/781   test_loss = 1.033 这是测试集 \n",
      "2021-04-23T00:38:16.766461: Epoch   1 Batch   39/781   test_loss = 0.853 这是测试集 \n",
      "2021-04-23T00:38:16.948424: Epoch   1 Batch   59/781   test_loss = 0.950 这是测试集 \n",
      "2021-04-23T00:38:17.135143: Epoch   1 Batch   79/781   test_loss = 0.982 这是测试集 \n",
      "2021-04-23T00:38:17.331318: Epoch   1 Batch   99/781   test_loss = 1.071 这是测试集 \n",
      "2021-04-23T00:38:17.522614: Epoch   1 Batch  119/781   test_loss = 0.988 这是测试集 \n",
      "2021-04-23T00:38:17.722694: Epoch   1 Batch  139/781   test_loss = 1.101 这是测试集 \n",
      "2021-04-23T00:38:17.911062: Epoch   1 Batch  159/781   test_loss = 1.069 这是测试集 \n",
      "2021-04-23T00:38:18.107239: Epoch   1 Batch  179/781   test_loss = 0.965 这是测试集 \n",
      "2021-04-23T00:38:18.289750: Epoch   1 Batch  199/781   test_loss = 0.934 这是测试集 \n",
      "2021-04-23T00:38:18.464759: Epoch   1 Batch  219/781   test_loss = 1.050 这是测试集 \n",
      "2021-04-23T00:38:18.669368: Epoch   1 Batch  239/781   test_loss = 1.197 这是测试集 \n",
      "2021-04-23T00:38:18.865543: Epoch   1 Batch  259/781   test_loss = 0.986 这是测试集 \n",
      "2021-04-23T00:38:19.066867: Epoch   1 Batch  279/781   test_loss = 1.217 这是测试集 \n",
      "2021-04-23T00:38:19.267632: Epoch   1 Batch  299/781   test_loss = 1.194 这是测试集 \n",
      "2021-04-23T00:38:19.494908: Epoch   1 Batch  319/781   test_loss = 1.031 这是测试集 \n",
      "2021-04-23T00:38:19.701822: Epoch   1 Batch  339/781   test_loss = 0.944 这是测试集 \n",
      "2021-04-23T00:38:19.893115: Epoch   1 Batch  359/781   test_loss = 0.921 这是测试集 \n",
      "2021-04-23T00:38:20.094171: Epoch   1 Batch  379/781   test_loss = 1.038 这是测试集 \n",
      "2021-04-23T00:38:20.301082: Epoch   1 Batch  399/781   test_loss = 0.924 这是测试集 \n",
      "2021-04-23T00:38:20.469930: Epoch   1 Batch  419/781   test_loss = 0.936 这是测试集 \n",
      "2021-04-23T00:38:20.684650: Epoch   1 Batch  439/781   test_loss = 1.033 这是测试集 \n",
      "2021-04-23T00:38:20.896446: Epoch   1 Batch  459/781   test_loss = 1.153 这是测试集 \n",
      "2021-04-23T00:38:21.092231: Epoch   1 Batch  479/781   test_loss = 1.115 这是测试集 \n",
      "2021-04-23T00:38:21.279621: Epoch   1 Batch  499/781   test_loss = 0.907 这是测试集 \n",
      "2021-04-23T00:38:21.483159: Epoch   1 Batch  519/781   test_loss = 1.128 这是测试集 \n",
      "2021-04-23T00:38:21.675433: Epoch   1 Batch  539/781   test_loss = 0.936 这是测试集 \n",
      "2021-04-23T00:38:21.865750: Epoch   1 Batch  559/781   test_loss = 1.195 这是测试集 \n",
      "2021-04-23T00:38:22.064854: Epoch   1 Batch  579/781   test_loss = 1.133 这是测试集 \n",
      "2021-04-23T00:38:22.257125: Epoch   1 Batch  599/781   test_loss = 1.026 这是测试集 \n",
      "2021-04-23T00:38:22.432808: Epoch   1 Batch  619/781   test_loss = 1.196 这是测试集 \n",
      "2021-04-23T00:38:22.620198: Epoch   1 Batch  639/781   test_loss = 0.913 这是测试集 \n",
      "2021-04-23T00:38:22.835894: Epoch   1 Batch  659/781   test_loss = 1.120 这是测试集 \n",
      "2021-04-23T00:38:23.032984: Epoch   1 Batch  679/781   test_loss = 1.172 这是测试集 \n",
      "2021-04-23T00:38:23.228157: Epoch   1 Batch  699/781   test_loss = 0.925 这是测试集 \n",
      "2021-04-23T00:38:23.421852: Epoch   1 Batch  719/781   test_loss = 1.034 这是测试集 \n",
      "2021-04-23T00:38:23.624392: Epoch   1 Batch  739/781   test_loss = 1.018 这是测试集 \n",
      "2021-04-23T00:38:23.840088: Epoch   1 Batch  759/781   test_loss = 0.991 这是测试集 \n",
      "2021-04-23T00:38:24.047977: Epoch   1 Batch  779/781   test_loss = 0.817 这是测试集 \n",
      "2021-04-23T00:38:25.409560: Epoch   2 Batch   10/3125   train_loss = 0.952 这是训练集\n",
      "2021-04-23T00:38:26.009674: Epoch   2 Batch   30/3125   train_loss = 1.024 这是训练集\n",
      "2021-04-23T00:38:26.598202: Epoch   2 Batch   50/3125   train_loss = 1.079 这是训练集\n",
      "2021-04-23T00:38:27.241424: Epoch   2 Batch   70/3125   train_loss = 1.079 这是训练集\n",
      "2021-04-23T00:38:27.867997: Epoch   2 Batch   90/3125   train_loss = 1.008 这是训练集\n",
      "2021-04-23T00:38:28.458557: Epoch   2 Batch  110/3125   train_loss = 0.899 这是训练集\n",
      "2021-04-23T00:38:29.056844: Epoch   2 Batch  130/3125   train_loss = 0.964 这是训练集\n",
      "2021-04-23T00:38:29.675665: Epoch   2 Batch  150/3125   train_loss = 1.097 这是训练集\n",
      "2021-04-23T00:38:30.277806: Epoch   2 Batch  170/3125   train_loss = 1.018 这是训练集\n",
      "2021-04-23T00:38:30.883212: Epoch   2 Batch  190/3125   train_loss = 1.059 这是训练集\n",
      "2021-04-23T00:38:31.487862: Epoch   2 Batch  210/3125   train_loss = 1.011 这是训练集\n",
      "2021-04-23T00:38:32.078341: Epoch   2 Batch  230/3125   train_loss = 1.108 这是训练集\n",
      "2021-04-23T00:38:32.631773: Epoch   2 Batch  250/3125   train_loss = 0.961 这是训练集\n",
      "2021-04-23T00:38:33.225834: Epoch   2 Batch  270/3125   train_loss = 0.818 这是训练集\n",
      "2021-04-23T00:38:33.828026: Epoch   2 Batch  290/3125   train_loss = 1.153 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:38:34.409251: Epoch   2 Batch  310/3125   train_loss = 1.065 这是训练集\n",
      "2021-04-23T00:38:35.022645: Epoch   2 Batch  330/3125   train_loss = 1.041 这是训练集\n",
      "2021-04-23T00:38:35.648262: Epoch   2 Batch  350/3125   train_loss = 0.926 这是训练集\n",
      "2021-04-23T00:38:36.285120: Epoch   2 Batch  370/3125   train_loss = 1.249 这是训练集\n",
      "2021-04-23T00:38:36.868889: Epoch   2 Batch  390/3125   train_loss = 1.178 这是训练集\n",
      "2021-04-23T00:38:37.492676: Epoch   2 Batch  410/3125   train_loss = 0.949 这是训练集\n",
      "2021-04-23T00:38:38.096820: Epoch   2 Batch  430/3125   train_loss = 1.149 这是训练集\n",
      "2021-04-23T00:38:38.691251: Epoch   2 Batch  450/3125   train_loss = 0.964 这是训练集\n",
      "2021-04-23T00:38:39.425791: Epoch   2 Batch  470/3125   train_loss = 0.977 这是训练集\n",
      "2021-04-23T00:38:40.163646: Epoch   2 Batch  490/3125   train_loss = 1.018 这是训练集\n",
      "2021-04-23T00:38:40.723060: Epoch   2 Batch  510/3125   train_loss = 1.100 这是训练集\n",
      "2021-04-23T00:38:41.329156: Epoch   2 Batch  530/3125   train_loss = 1.029 这是训练集\n",
      "2021-04-23T00:38:41.924514: Epoch   2 Batch  550/3125   train_loss = 0.999 这是训练集\n",
      "2021-04-23T00:38:42.489084: Epoch   2 Batch  570/3125   train_loss = 1.087 这是训练集\n",
      "2021-04-23T00:38:43.104830: Epoch   2 Batch  590/3125   train_loss = 1.084 这是训练集\n",
      "2021-04-23T00:38:43.700997: Epoch   2 Batch  610/3125   train_loss = 0.979 这是训练集\n",
      "2021-04-23T00:38:44.291476: Epoch   2 Batch  630/3125   train_loss = 1.090 这是训练集\n",
      "2021-04-23T00:38:44.839992: Epoch   2 Batch  650/3125   train_loss = 1.059 这是训练集\n",
      "2021-04-23T00:38:45.424298: Epoch   2 Batch  670/3125   train_loss = 0.977 这是训练集\n",
      "2021-04-23T00:38:46.001189: Epoch   2 Batch  690/3125   train_loss = 0.942 这是训练集\n",
      "2021-04-23T00:38:46.551652: Epoch   2 Batch  710/3125   train_loss = 0.968 这是训练集\n",
      "2021-04-23T00:38:47.157280: Epoch   2 Batch  730/3125   train_loss = 0.889 这是训练集\n",
      "2021-04-23T00:38:47.765309: Epoch   2 Batch  750/3125   train_loss = 0.995 这是训练集\n",
      "2021-04-23T00:38:48.377688: Epoch   2 Batch  770/3125   train_loss = 0.895 这是训练集\n",
      "2021-04-23T00:38:48.958408: Epoch   2 Batch  790/3125   train_loss = 0.864 这是训练集\n",
      "2021-04-23T00:38:49.568486: Epoch   2 Batch  810/3125   train_loss = 0.876 这是训练集\n",
      "2021-04-23T00:38:50.503146: Epoch   2 Batch  830/3125   train_loss = 0.834 这是训练集\n",
      "2021-04-23T00:38:51.095578: Epoch   2 Batch  850/3125   train_loss = 1.057 这是训练集\n",
      "2021-04-23T00:38:51.709481: Epoch   2 Batch  870/3125   train_loss = 0.936 这是训练集\n",
      "2021-04-23T00:38:52.313371: Epoch   2 Batch  890/3125   train_loss = 0.859 这是训练集\n",
      "2021-04-23T00:38:52.870588: Epoch   2 Batch  910/3125   train_loss = 0.939 这是训练集\n",
      "2021-04-23T00:38:53.441078: Epoch   2 Batch  930/3125   train_loss = 1.058 这是训练集\n",
      "2021-04-23T00:38:54.018870: Epoch   2 Batch  950/3125   train_loss = 0.932 这是训练集\n",
      "2021-04-23T00:38:54.570358: Epoch   2 Batch  970/3125   train_loss = 1.026 这是训练集\n",
      "2021-04-23T00:38:55.173587: Epoch   2 Batch  990/3125   train_loss = 0.881 这是训练集\n",
      "2021-04-23T00:38:55.775778: Epoch   2 Batch 1010/3125   train_loss = 1.117 这是训练集\n",
      "2021-04-23T00:38:56.348688: Epoch   2 Batch 1030/3125   train_loss = 0.917 这是训练集\n",
      "2021-04-23T00:38:56.919179: Epoch   2 Batch 1050/3125   train_loss = 0.907 这是训练集\n",
      "2021-04-23T00:38:57.514625: Epoch   2 Batch 1070/3125   train_loss = 1.049 这是训练集\n",
      "2021-04-23T00:38:58.106204: Epoch   2 Batch 1090/3125   train_loss = 1.065 这是训练集\n",
      "2021-04-23T00:38:58.670369: Epoch   2 Batch 1110/3125   train_loss = 1.103 这是训练集\n",
      "2021-04-23T00:38:59.261827: Epoch   2 Batch 1130/3125   train_loss = 0.930 这是训练集\n",
      "2021-04-23T00:38:59.846539: Epoch   2 Batch 1150/3125   train_loss = 1.005 这是训练集\n",
      "2021-04-23T00:39:00.461755: Epoch   2 Batch 1170/3125   train_loss = 1.027 这是训练集\n",
      "2021-04-23T00:39:01.056150: Epoch   2 Batch 1190/3125   train_loss = 1.039 这是训练集\n",
      "2021-04-23T00:39:01.644666: Epoch   2 Batch 1210/3125   train_loss = 0.919 这是训练集\n",
      "2021-04-23T00:39:02.229798: Epoch   2 Batch 1230/3125   train_loss = 0.888 这是训练集\n",
      "2021-04-23T00:39:02.777334: Epoch   2 Batch 1250/3125   train_loss = 0.966 这是训练集\n",
      "2021-04-23T00:39:03.361957: Epoch   2 Batch 1270/3125   train_loss = 1.022 这是训练集\n",
      "2021-04-23T00:39:03.953419: Epoch   2 Batch 1290/3125   train_loss = 1.009 这是训练集\n",
      "2021-04-23T00:39:04.514464: Epoch   2 Batch 1310/3125   train_loss = 0.973 这是训练集\n",
      "2021-04-23T00:39:05.098112: Epoch   2 Batch 1330/3125   train_loss = 1.100 这是训练集\n",
      "2021-04-23T00:39:05.729560: Epoch   2 Batch 1350/3125   train_loss = 0.897 这是训练集\n",
      "2021-04-23T00:39:06.339632: Epoch   2 Batch 1370/3125   train_loss = 0.860 这是训练集\n",
      "2021-04-23T00:39:06.962321: Epoch   2 Batch 1390/3125   train_loss = 1.048 这是训练集\n",
      "2021-04-23T00:39:07.637293: Epoch   2 Batch 1410/3125   train_loss = 0.949 这是训练集\n",
      "2021-04-23T00:39:08.231634: Epoch   2 Batch 1430/3125   train_loss = 1.002 这是训练集\n",
      "2021-04-23T00:39:08.821595: Epoch   2 Batch 1450/3125   train_loss = 1.028 这是训练集\n",
      "2021-04-23T00:39:09.451115: Epoch   2 Batch 1470/3125   train_loss = 1.012 这是训练集\n",
      "2021-04-23T00:39:10.055258: Epoch   2 Batch 1490/3125   train_loss = 1.035 这是训练集\n",
      "2021-04-23T00:39:10.621943: Epoch   2 Batch 1510/3125   train_loss = 0.986 这是训练集\n",
      "2021-04-23T00:39:11.197023: Epoch   2 Batch 1530/3125   train_loss = 1.102 这是训练集\n",
      "2021-04-23T00:39:11.793865: Epoch   2 Batch 1550/3125   train_loss = 0.851 这是训练集\n",
      "2021-04-23T00:39:12.354088: Epoch   2 Batch 1570/3125   train_loss = 0.950 这是训练集\n",
      "2021-04-23T00:39:12.918216: Epoch   2 Batch 1590/3125   train_loss = 1.012 这是训练集\n",
      "2021-04-23T00:39:13.511702: Epoch   2 Batch 1610/3125   train_loss = 1.046 这是训练集\n",
      "2021-04-23T00:39:14.111476: Epoch   2 Batch 1630/3125   train_loss = 1.040 这是训练集\n",
      "2021-04-23T00:39:14.682435: Epoch   2 Batch 1650/3125   train_loss = 0.883 这是训练集\n",
      "2021-04-23T00:39:15.286811: Epoch   2 Batch 1670/3125   train_loss = 0.815 这是训练集\n",
      "2021-04-23T00:39:15.932419: Epoch   2 Batch 1690/3125   train_loss = 0.976 这是训练集\n",
      "2021-04-23T00:39:16.521922: Epoch   2 Batch 1710/3125   train_loss = 0.935 这是训练集\n",
      "2021-04-23T00:39:17.144146: Epoch   2 Batch 1730/3125   train_loss = 1.007 这是训练集\n",
      "2021-04-23T00:39:17.770089: Epoch   2 Batch 1750/3125   train_loss = 0.835 这是训练集\n",
      "2021-04-23T00:39:18.362642: Epoch   2 Batch 1770/3125   train_loss = 1.129 这是训练集\n",
      "2021-04-23T00:39:18.946369: Epoch   2 Batch 1790/3125   train_loss = 1.043 这是训练集\n",
      "2021-04-23T00:39:19.544657: Epoch   2 Batch 1810/3125   train_loss = 0.981 这是训练集\n",
      "2021-04-23T00:39:20.179891: Epoch   2 Batch 1830/3125   train_loss = 1.011 这是训练集\n",
      "2021-04-23T00:39:20.728922: Epoch   2 Batch 1850/3125   train_loss = 0.958 这是训练集\n",
      "2021-04-23T00:39:21.310147: Epoch   2 Batch 1870/3125   train_loss = 1.039 这是训练集\n",
      "2021-04-23T00:39:21.903554: Epoch   2 Batch 1890/3125   train_loss = 0.848 这是训练集\n",
      "2021-04-23T00:39:22.466461: Epoch   2 Batch 1910/3125   train_loss = 0.929 这是训练集\n",
      "2021-04-23T00:39:23.050193: Epoch   2 Batch 1930/3125   train_loss = 0.977 这是训练集\n",
      "2021-04-23T00:39:23.687521: Epoch   2 Batch 1950/3125   train_loss = 0.970 这是训练集\n",
      "2021-04-23T00:39:24.305368: Epoch   2 Batch 1970/3125   train_loss = 0.954 这是训练集\n",
      "2021-04-23T00:39:24.894575: Epoch   2 Batch 1990/3125   train_loss = 0.841 这是训练集\n",
      "2021-04-23T00:39:25.503127: Epoch   2 Batch 2010/3125   train_loss = 0.845 这是训练集\n",
      "2021-04-23T00:39:26.118007: Epoch   2 Batch 2030/3125   train_loss = 0.939 这是训练集\n",
      "2021-04-23T00:39:26.710836: Epoch   2 Batch 2050/3125   train_loss = 0.961 这是训练集\n",
      "2021-04-23T00:39:27.314042: Epoch   2 Batch 2070/3125   train_loss = 0.933 这是训练集\n",
      "2021-04-23T00:39:27.938681: Epoch   2 Batch 2090/3125   train_loss = 0.883 这是训练集\n",
      "2021-04-23T00:39:28.519401: Epoch   2 Batch 2110/3125   train_loss = 1.079 这是训练集\n",
      "2021-04-23T00:39:29.176340: Epoch   2 Batch 2130/3125   train_loss = 0.994 这是训练集\n",
      "2021-04-23T00:39:29.778471: Epoch   2 Batch 2150/3125   train_loss = 0.983 这是训练集\n",
      "2021-04-23T00:39:30.361222: Epoch   2 Batch 2170/3125   train_loss = 0.873 这是训练集\n",
      "2021-04-23T00:39:30.927301: Epoch   2 Batch 2190/3125   train_loss = 0.963 这是训练集\n",
      "2021-04-23T00:39:31.522193: Epoch   2 Batch 2210/3125   train_loss = 1.014 这是训练集\n",
      "2021-04-23T00:39:32.117124: Epoch   2 Batch 2230/3125   train_loss = 0.865 这是训练集\n",
      "2021-04-23T00:39:32.683204: Epoch   2 Batch 2250/3125   train_loss = 0.962 这是训练集\n",
      "2021-04-23T00:39:33.285394: Epoch   2 Batch 2270/3125   train_loss = 0.966 这是训练集\n",
      "2021-04-23T00:39:33.892038: Epoch   2 Batch 2290/3125   train_loss = 0.855 这是训练集\n",
      "2021-04-23T00:39:34.461419: Epoch   2 Batch 2310/3125   train_loss = 0.924 这是训练集\n",
      "2021-04-23T00:39:35.057755: Epoch   2 Batch 2330/3125   train_loss = 1.037 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:39:35.719482: Epoch   2 Batch 2350/3125   train_loss = 1.036 这是训练集\n",
      "2021-04-23T00:39:36.315014: Epoch   2 Batch 2370/3125   train_loss = 0.981 这是训练集\n",
      "2021-04-23T00:39:36.890968: Epoch   2 Batch 2390/3125   train_loss = 1.006 这是训练集\n",
      "2021-04-23T00:39:37.497063: Epoch   2 Batch 2410/3125   train_loss = 1.094 这是训练集\n",
      "2021-04-23T00:39:38.109015: Epoch   2 Batch 2430/3125   train_loss = 0.907 这是训练集\n",
      "2021-04-23T00:39:38.676109: Epoch   2 Batch 2450/3125   train_loss = 1.015 这是训练集\n",
      "2021-04-23T00:39:39.420796: Epoch   2 Batch 2470/3125   train_loss = 1.009 这是训练集\n",
      "2021-04-23T00:39:40.242119: Epoch   2 Batch 2490/3125   train_loss = 1.025 这是训练集\n",
      "2021-04-23T00:39:41.019154: Epoch   2 Batch 2510/3125   train_loss = 1.044 这是训练集\n",
      "2021-04-23T00:39:41.740026: Epoch   2 Batch 2530/3125   train_loss = 0.813 这是训练集\n",
      "2021-04-23T00:39:42.443721: Epoch   2 Batch 2550/3125   train_loss = 1.055 这是训练集\n",
      "2021-04-23T00:39:43.155169: Epoch   2 Batch 2570/3125   train_loss = 0.995 这是训练集\n",
      "2021-04-23T00:39:43.784916: Epoch   2 Batch 2590/3125   train_loss = 0.940 这是训练集\n",
      "2021-04-23T00:39:44.374926: Epoch   2 Batch 2610/3125   train_loss = 1.014 这是训练集\n",
      "2021-04-23T00:39:44.943933: Epoch   2 Batch 2630/3125   train_loss = 0.711 这是训练集\n",
      "2021-04-23T00:39:45.569158: Epoch   2 Batch 2650/3125   train_loss = 0.907 这是训练集\n",
      "2021-04-23T00:39:46.219178: Epoch   2 Batch 2670/3125   train_loss = 1.033 这是训练集\n",
      "2021-04-23T00:39:46.816359: Epoch   2 Batch 2690/3125   train_loss = 0.979 这是训练集\n",
      "2021-04-23T00:39:47.437095: Epoch   2 Batch 2710/3125   train_loss = 0.836 这是训练集\n",
      "2021-04-23T00:39:48.036361: Epoch   2 Batch 2730/3125   train_loss = 1.082 这是训练集\n",
      "2021-04-23T00:39:48.619280: Epoch   2 Batch 2750/3125   train_loss = 0.987 这是训练集\n",
      "2021-04-23T00:39:49.207848: Epoch   2 Batch 2770/3125   train_loss = 0.922 这是训练集\n",
      "2021-04-23T00:39:49.788567: Epoch   2 Batch 2790/3125   train_loss = 0.902 这是训练集\n",
      "2021-04-23T00:39:50.688127: Epoch   2 Batch 2810/3125   train_loss = 0.953 这是训练集\n",
      "2021-04-23T00:39:51.265276: Epoch   2 Batch 2830/3125   train_loss = 0.868 这是训练集\n",
      "2021-04-23T00:39:51.848923: Epoch   2 Batch 2850/3125   train_loss = 1.040 这是训练集\n",
      "2021-04-23T00:39:52.429642: Epoch   2 Batch 2870/3125   train_loss = 0.852 这是训练集\n",
      "2021-04-23T00:39:53.000138: Epoch   2 Batch 2890/3125   train_loss = 0.775 这是训练集\n",
      "2021-04-23T00:39:53.595119: Epoch   2 Batch 2910/3125   train_loss = 1.013 这是训练集\n",
      "2021-04-23T00:39:54.180252: Epoch   2 Batch 2930/3125   train_loss = 0.822 这是训练集\n",
      "2021-04-23T00:39:54.726811: Epoch   2 Batch 2950/3125   train_loss = 1.065 这是训练集\n",
      "2021-04-23T00:39:55.320080: Epoch   2 Batch 2970/3125   train_loss = 0.936 这是训练集\n",
      "2021-04-23T00:39:55.971335: Epoch   2 Batch 2990/3125   train_loss = 0.889 这是训练集\n",
      "2021-04-23T00:39:56.566695: Epoch   2 Batch 3010/3125   train_loss = 0.958 这是训练集\n",
      "2021-04-23T00:39:57.189384: Epoch   2 Batch 3030/3125   train_loss = 1.023 这是训练集\n",
      "2021-04-23T00:39:57.805661: Epoch   2 Batch 3050/3125   train_loss = 0.952 这是训练集\n",
      "2021-04-23T00:39:58.370840: Epoch   2 Batch 3070/3125   train_loss = 0.901 这是训练集\n",
      "2021-04-23T00:39:58.929112: Epoch   2 Batch 3090/3125   train_loss = 0.813 这是训练集\n",
      "2021-04-23T00:39:59.518620: Epoch   2 Batch 3110/3125   train_loss = 0.874 这是训练集\n",
      "2021-04-23T00:40:00.126707: Epoch   2 Batch   18/781   test_loss = 0.867 这是测试集 \n",
      "2021-04-23T00:40:00.320930: Epoch   2 Batch   38/781   test_loss = 0.852 这是测试集 \n",
      "2021-04-23T00:40:00.495634: Epoch   2 Batch   58/781   test_loss = 0.906 这是测试集 \n",
      "2021-04-23T00:40:00.687906: Epoch   2 Batch   78/781   test_loss = 0.889 这是测试集 \n",
      "2021-04-23T00:40:00.878227: Epoch   2 Batch   98/781   test_loss = 0.963 这是测试集 \n",
      "2021-04-23T00:40:01.065618: Epoch   2 Batch  118/781   test_loss = 0.856 这是测试集 \n",
      "2021-04-23T00:40:01.259373: Epoch   2 Batch  138/781   test_loss = 0.996 这是测试集 \n",
      "2021-04-23T00:40:01.457502: Epoch   2 Batch  158/781   test_loss = 0.897 这是测试集 \n",
      "2021-04-23T00:40:01.652705: Epoch   2 Batch  178/781   test_loss = 0.857 这是测试集 \n",
      "2021-04-23T00:40:01.847269: Epoch   2 Batch  198/781   test_loss = 0.929 这是测试集 \n",
      "2021-04-23T00:40:02.037569: Epoch   2 Batch  218/781   test_loss = 1.004 这是测试集 \n",
      "2021-04-23T00:40:02.236083: Epoch   2 Batch  238/781   test_loss = 0.987 这是测试集 \n",
      "2021-04-23T00:40:02.410787: Epoch   2 Batch  258/781   test_loss = 0.936 这是测试集 \n",
      "2021-04-23T00:40:02.593299: Epoch   2 Batch  278/781   test_loss = 1.058 这是测试集 \n",
      "2021-04-23T00:40:02.789475: Epoch   2 Batch  298/781   test_loss = 0.941 这是测试集 \n",
      "2021-04-23T00:40:02.980771: Epoch   2 Batch  318/781   test_loss = 0.914 这是测试集 \n",
      "2021-04-23T00:40:03.174995: Epoch   2 Batch  338/781   test_loss = 0.954 这是测试集 \n",
      "2021-04-23T00:40:03.377026: Epoch   2 Batch  358/781   test_loss = 0.931 这是测试集 \n",
      "2021-04-23T00:40:03.580034: Epoch   2 Batch  378/781   test_loss = 0.859 这是测试集 \n",
      "2021-04-23T00:40:03.777186: Epoch   2 Batch  398/781   test_loss = 0.849 这是测试集 \n",
      "2021-04-23T00:40:03.975313: Epoch   2 Batch  418/781   test_loss = 0.971 这是测试集 \n",
      "2021-04-23T00:40:04.170046: Epoch   2 Batch  438/781   test_loss = 1.030 这是测试集 \n",
      "2021-04-23T00:40:04.356280: Epoch   2 Batch  458/781   test_loss = 0.921 这是测试集 \n",
      "2021-04-23T00:40:04.535395: Epoch   2 Batch  478/781   test_loss = 0.955 这是测试集 \n",
      "2021-04-23T00:40:04.736451: Epoch   2 Batch  498/781   test_loss = 0.841 这是测试集 \n",
      "2021-04-23T00:40:04.931652: Epoch   2 Batch  518/781   test_loss = 0.902 这是测试集 \n",
      "2021-04-23T00:40:05.132707: Epoch   2 Batch  538/781   test_loss = 0.850 这是测试集 \n",
      "2021-04-23T00:40:05.333765: Epoch   2 Batch  558/781   test_loss = 0.906 这是测试集 \n",
      "2021-04-23T00:40:05.534819: Epoch   2 Batch  578/781   test_loss = 0.903 这是测试集 \n",
      "2021-04-23T00:40:05.734898: Epoch   2 Batch  598/781   test_loss = 1.136 这是测试集 \n",
      "2021-04-23T00:40:05.928655: Epoch   2 Batch  618/781   test_loss = 0.835 这是测试集 \n",
      "2021-04-23T00:40:06.125411: Epoch   2 Batch  638/781   test_loss = 0.897 这是测试集 \n",
      "2021-04-23T00:40:06.347572: Epoch   2 Batch  658/781   test_loss = 0.999 这是测试集 \n",
      "2021-04-23T00:40:06.551559: Epoch   2 Batch  678/781   test_loss = 0.932 这是测试集 \n",
      "2021-04-23T00:40:06.752614: Epoch   2 Batch  698/781   test_loss = 0.880 这是测试集 \n",
      "2021-04-23T00:40:06.960503: Epoch   2 Batch  718/781   test_loss = 1.090 这是测试集 \n",
      "2021-04-23T00:40:07.150822: Epoch   2 Batch  738/781   test_loss = 0.862 这是测试集 \n",
      "2021-04-23T00:40:07.346998: Epoch   2 Batch  758/781   test_loss = 1.002 这是测试集 \n",
      "2021-04-23T00:40:07.544150: Epoch   2 Batch  778/781   test_loss = 0.930 这是测试集 \n",
      "2021-04-23T00:40:08.816392: Epoch   3 Batch    5/3125   train_loss = 0.955 这是训练集\n",
      "2021-04-23T00:40:09.506987: Epoch   3 Batch   25/3125   train_loss = 0.985 这是训练集\n",
      "2021-04-23T00:40:10.240045: Epoch   3 Batch   45/3125   train_loss = 0.891 这是训练集\n",
      "2021-04-23T00:40:10.939836: Epoch   3 Batch   65/3125   train_loss = 0.975 这是训练集\n",
      "2021-04-23T00:40:11.688854: Epoch   3 Batch   85/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:40:12.398995: Epoch   3 Batch  105/3125   train_loss = 0.735 这是训练集\n",
      "2021-04-23T00:40:13.123812: Epoch   3 Batch  125/3125   train_loss = 0.911 这是训练集\n",
      "2021-04-23T00:40:13.859292: Epoch   3 Batch  145/3125   train_loss = 0.989 这是训练集\n",
      "2021-04-23T00:40:14.523946: Epoch   3 Batch  165/3125   train_loss = 0.897 这是训练集\n",
      "2021-04-23T00:40:15.208712: Epoch   3 Batch  185/3125   train_loss = 0.844 这是训练集\n",
      "2021-04-23T00:40:15.846202: Epoch   3 Batch  205/3125   train_loss = 0.853 这是训练集\n",
      "2021-04-23T00:40:16.446441: Epoch   3 Batch  225/3125   train_loss = 0.834 这是训练集\n",
      "2021-04-23T00:40:17.056478: Epoch   3 Batch  245/3125   train_loss = 1.085 这是训练集\n",
      "2021-04-23T00:40:17.665105: Epoch   3 Batch  265/3125   train_loss = 0.846 这是训练集\n",
      "2021-04-23T00:40:18.291048: Epoch   3 Batch  285/3125   train_loss = 0.926 这是训练集\n",
      "2021-04-23T00:40:18.918617: Epoch   3 Batch  305/3125   train_loss = 0.826 这是训练集\n",
      "2021-04-23T00:40:19.523735: Epoch   3 Batch  325/3125   train_loss = 0.961 这是训练集\n",
      "2021-04-23T00:40:20.187490: Epoch   3 Batch  345/3125   train_loss = 0.960 这是训练集\n",
      "2021-04-23T00:40:20.741388: Epoch   3 Batch  365/3125   train_loss = 0.864 这是训练集\n",
      "2021-04-23T00:40:21.341627: Epoch   3 Batch  385/3125   train_loss = 0.853 这是训练集\n",
      "2021-04-23T00:40:21.957564: Epoch   3 Batch  405/3125   train_loss = 0.863 这是训练集\n",
      "2021-04-23T00:40:22.506547: Epoch   3 Batch  425/3125   train_loss = 0.963 这是训练集\n",
      "2021-04-23T00:40:23.108854: Epoch   3 Batch  445/3125   train_loss = 0.975 这是训练集\n",
      "2021-04-23T00:40:23.698357: Epoch   3 Batch  465/3125   train_loss = 0.924 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:40:24.259556: Epoch   3 Batch  485/3125   train_loss = 0.979 这是训练集\n",
      "2021-04-23T00:40:24.815750: Epoch   3 Batch  505/3125   train_loss = 0.854 这是训练集\n",
      "2021-04-23T00:40:25.402860: Epoch   3 Batch  525/3125   train_loss = 1.026 这是训练集\n",
      "2021-04-23T00:40:26.050923: Epoch   3 Batch  545/3125   train_loss = 0.869 这是训练集\n",
      "2021-04-23T00:40:26.596215: Epoch   3 Batch  565/3125   train_loss = 1.106 这是训练集\n",
      "2021-04-23T00:40:27.226467: Epoch   3 Batch  585/3125   train_loss = 0.850 这是训练集\n",
      "2021-04-23T00:40:27.833538: Epoch   3 Batch  605/3125   train_loss = 0.920 这是训练集\n",
      "2021-04-23T00:40:28.416209: Epoch   3 Batch  625/3125   train_loss = 0.905 这是训练集\n",
      "2021-04-23T00:40:29.002415: Epoch   3 Batch  645/3125   train_loss = 0.969 这是训练集\n",
      "2021-04-23T00:40:29.596372: Epoch   3 Batch  665/3125   train_loss = 1.059 这是训练集\n",
      "2021-04-23T00:40:30.191853: Epoch   3 Batch  685/3125   train_loss = 0.949 这是训练集\n",
      "2021-04-23T00:40:30.766246: Epoch   3 Batch  705/3125   train_loss = 1.106 这是训练集\n",
      "2021-04-23T00:40:31.360629: Epoch   3 Batch  725/3125   train_loss = 0.905 这是训练集\n",
      "2021-04-23T00:40:31.963853: Epoch   3 Batch  745/3125   train_loss = 0.871 这是训练集\n",
      "2021-04-23T00:40:32.526405: Epoch   3 Batch  765/3125   train_loss = 0.889 这是训练集\n",
      "2021-04-23T00:40:33.132858: Epoch   3 Batch  785/3125   train_loss = 1.066 这是训练集\n",
      "2021-04-23T00:40:33.737001: Epoch   3 Batch  805/3125   train_loss = 0.858 这是训练集\n",
      "2021-04-23T00:40:34.313816: Epoch   3 Batch  825/3125   train_loss = 0.926 这是训练集\n",
      "2021-04-23T00:40:34.871291: Epoch   3 Batch  845/3125   train_loss = 0.920 这是训练集\n",
      "2021-04-23T00:40:35.457945: Epoch   3 Batch  865/3125   train_loss = 1.016 这是训练集\n",
      "2021-04-23T00:40:36.065993: Epoch   3 Batch  885/3125   train_loss = 0.914 这是训练集\n",
      "2021-04-23T00:40:36.614505: Epoch   3 Batch  905/3125   train_loss = 0.944 这是训练集\n",
      "2021-04-23T00:40:37.218792: Epoch   3 Batch  925/3125   train_loss = 0.956 这是训练集\n",
      "2021-04-23T00:40:37.813797: Epoch   3 Batch  945/3125   train_loss = 0.968 这是训练集\n",
      "2021-04-23T00:40:38.384757: Epoch   3 Batch  965/3125   train_loss = 0.801 这是训练集\n",
      "2021-04-23T00:40:38.952831: Epoch   3 Batch  985/3125   train_loss = 0.967 这是训练集\n",
      "2021-04-23T00:40:39.543735: Epoch   3 Batch 1005/3125   train_loss = 0.796 这是训练集\n",
      "2021-04-23T00:40:40.157678: Epoch   3 Batch 1025/3125   train_loss = 0.897 这是训练集\n",
      "2021-04-23T00:40:40.713021: Epoch   3 Batch 1045/3125   train_loss = 1.172 这是训练集\n",
      "2021-04-23T00:40:41.325987: Epoch   3 Batch 1065/3125   train_loss = 0.886 这是训练集\n",
      "2021-04-23T00:40:41.945281: Epoch   3 Batch 1085/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:40:42.501132: Epoch   3 Batch 1105/3125   train_loss = 0.806 这是训练集\n",
      "2021-04-23T00:40:43.097468: Epoch   3 Batch 1125/3125   train_loss = 0.827 这是训练集\n",
      "2021-04-23T00:40:43.694817: Epoch   3 Batch 1145/3125   train_loss = 0.902 这是训练集\n",
      "2021-04-23T00:40:44.305836: Epoch   3 Batch 1165/3125   train_loss = 1.011 这是训练集\n",
      "2021-04-23T00:40:44.900219: Epoch   3 Batch 1185/3125   train_loss = 0.855 这是训练集\n",
      "2021-04-23T00:40:45.492650: Epoch   3 Batch 1205/3125   train_loss = 0.891 这是训练集\n",
      "2021-04-23T00:40:46.082503: Epoch   3 Batch 1225/3125   train_loss = 0.989 这是训练集\n",
      "2021-04-23T00:40:46.623793: Epoch   3 Batch 1245/3125   train_loss = 1.023 这是训练集\n",
      "2021-04-23T00:40:47.224072: Epoch   3 Batch 1265/3125   train_loss = 0.940 这是训练集\n",
      "2021-04-23T00:40:47.837975: Epoch   3 Batch 1285/3125   train_loss = 0.953 这是训练集\n",
      "2021-04-23T00:40:48.411301: Epoch   3 Batch 1305/3125   train_loss = 0.808 这是训练集\n",
      "2021-04-23T00:40:49.060522: Epoch   3 Batch 1325/3125   train_loss = 0.903 这是训练集\n",
      "2021-04-23T00:40:49.669012: Epoch   3 Batch 1345/3125   train_loss = 0.917 这是训练集\n",
      "2021-04-23T00:40:50.548387: Epoch   3 Batch 1365/3125   train_loss = 0.815 这是训练集\n",
      "2021-04-23T00:40:51.170662: Epoch   3 Batch 1385/3125   train_loss = 0.808 这是训练集\n",
      "2021-04-23T00:40:51.774204: Epoch   3 Batch 1405/3125   train_loss = 0.895 这是训练集\n",
      "2021-04-23T00:40:52.345163: Epoch   3 Batch 1425/3125   train_loss = 1.110 这是训练集\n",
      "2021-04-23T00:40:52.939121: Epoch   3 Batch 1445/3125   train_loss = 1.073 这是训练集\n",
      "2021-04-23T00:40:53.601550: Epoch   3 Batch 1465/3125   train_loss = 0.929 这是训练集\n",
      "2021-04-23T00:40:54.217447: Epoch   3 Batch 1485/3125   train_loss = 0.948 这是训练集\n",
      "2021-04-23T00:40:54.768886: Epoch   3 Batch 1505/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:40:55.402823: Epoch   3 Batch 1525/3125   train_loss = 0.764 这是训练集\n",
      "2021-04-23T00:40:55.989516: Epoch   3 Batch 1545/3125   train_loss = 0.869 这是训练集\n",
      "2021-04-23T00:40:56.544933: Epoch   3 Batch 1565/3125   train_loss = 0.980 这是训练集\n",
      "2021-04-23T00:40:57.187140: Epoch   3 Batch 1585/3125   train_loss = 0.861 这是训练集\n",
      "2021-04-23T00:40:57.841604: Epoch   3 Batch 1605/3125   train_loss = 0.934 这是训练集\n",
      "2021-04-23T00:40:58.444998: Epoch   3 Batch 1625/3125   train_loss = 0.978 这是训练集\n",
      "2021-04-23T00:40:59.118526: Epoch   3 Batch 1645/3125   train_loss = 1.046 这是训练集\n",
      "2021-04-23T00:40:59.725218: Epoch   3 Batch 1665/3125   train_loss = 0.918 这是训练集\n",
      "2021-04-23T00:41:00.375233: Epoch   3 Batch 1685/3125   train_loss = 1.027 这是训练集\n",
      "2021-04-23T00:41:00.960404: Epoch   3 Batch 1705/3125   train_loss = 0.931 这是训练集\n",
      "2021-04-23T00:41:01.566559: Epoch   3 Batch 1725/3125   train_loss = 0.819 这是训练集\n",
      "2021-04-23T00:41:02.153586: Epoch   3 Batch 1745/3125   train_loss = 0.807 这是训练集\n",
      "2021-04-23T00:41:02.705025: Epoch   3 Batch 1765/3125   train_loss = 0.838 这是训练集\n",
      "2021-04-23T00:41:03.291844: Epoch   3 Batch 1785/3125   train_loss = 1.063 这是训练集\n",
      "2021-04-23T00:41:03.895477: Epoch   3 Batch 1805/3125   train_loss = 0.984 这是训练集\n",
      "2021-04-23T00:41:04.466436: Epoch   3 Batch 1825/3125   train_loss = 0.945 这是训练集\n",
      "2021-04-23T00:41:05.050591: Epoch   3 Batch 1845/3125   train_loss = 0.949 这是训练集\n",
      "2021-04-23T00:41:05.646341: Epoch   3 Batch 1865/3125   train_loss = 0.751 这是训练集\n",
      "2021-04-23T00:41:06.240678: Epoch   3 Batch 1885/3125   train_loss = 0.978 这是训练集\n",
      "2021-04-23T00:41:06.818469: Epoch   3 Batch 1905/3125   train_loss = 0.822 这是训练集\n",
      "2021-04-23T00:41:07.442950: Epoch   3 Batch 1925/3125   train_loss = 0.849 这是训练集\n",
      "2021-04-23T00:41:08.096723: Epoch   3 Batch 1945/3125   train_loss = 0.924 这是训练集\n",
      "2021-04-23T00:41:08.667682: Epoch   3 Batch 1965/3125   train_loss = 0.877 这是训练集\n",
      "2021-04-23T00:41:09.264993: Epoch   3 Batch 1985/3125   train_loss = 0.892 这是训练集\n",
      "2021-04-23T00:41:09.864844: Epoch   3 Batch 2005/3125   train_loss = 0.914 这是训练集\n",
      "2021-04-23T00:41:10.432302: Epoch   3 Batch 2025/3125   train_loss = 0.979 这是训练集\n",
      "2021-04-23T00:41:11.021058: Epoch   3 Batch 2045/3125   train_loss = 0.785 这是训练集\n",
      "2021-04-23T00:41:11.625707: Epoch   3 Batch 2065/3125   train_loss = 0.752 这是训练集\n",
      "2021-04-23T00:41:12.229850: Epoch   3 Batch 2085/3125   train_loss = 0.979 这是训练集\n",
      "2021-04-23T00:41:12.784728: Epoch   3 Batch 2105/3125   train_loss = 0.867 这是训练集\n",
      "2021-04-23T00:41:13.366186: Epoch   3 Batch 2125/3125   train_loss = 0.945 这是训练集\n",
      "2021-04-23T00:41:14.023033: Epoch   3 Batch 2145/3125   train_loss = 1.004 这是训练集\n",
      "2021-04-23T00:41:14.825305: Epoch   3 Batch 2165/3125   train_loss = 0.837 这是训练集\n",
      "2021-04-23T00:41:15.535111: Epoch   3 Batch 2185/3125   train_loss = 0.960 这是训练集\n",
      "2021-04-23T00:41:16.194416: Epoch   3 Batch 2205/3125   train_loss = 0.943 这是训练集\n",
      "2021-04-23T00:41:16.784779: Epoch   3 Batch 2225/3125   train_loss = 0.846 这是训练集\n",
      "2021-04-23T00:41:17.391850: Epoch   3 Batch 2245/3125   train_loss = 0.832 这是训练集\n",
      "2021-04-23T00:41:17.999004: Epoch   3 Batch 2265/3125   train_loss = 0.922 这是训练集\n",
      "2021-04-23T00:41:18.556300: Epoch   3 Batch 2285/3125   train_loss = 1.116 这是训练集\n",
      "2021-04-23T00:41:19.159506: Epoch   3 Batch 2305/3125   train_loss = 0.849 这是训练集\n",
      "2021-04-23T00:41:19.776338: Epoch   3 Batch 2325/3125   train_loss = 0.812 这是训练集\n",
      "2021-04-23T00:41:20.371942: Epoch   3 Batch 2345/3125   train_loss = 0.877 这是训练集\n",
      "2021-04-23T00:41:20.947358: Epoch   3 Batch 2365/3125   train_loss = 0.740 这是训练集\n",
      "2021-04-23T00:41:21.627630: Epoch   3 Batch 2385/3125   train_loss = 0.916 这是训练集\n",
      "2021-04-23T00:41:22.239039: Epoch   3 Batch 2405/3125   train_loss = 0.883 这是训练集\n",
      "2021-04-23T00:41:22.795258: Epoch   3 Batch 2425/3125   train_loss = 0.904 这是训练集\n",
      "2021-04-23T00:41:23.411821: Epoch   3 Batch 2445/3125   train_loss = 0.963 这是训练集\n",
      "2021-04-23T00:41:24.024118: Epoch   3 Batch 2465/3125   train_loss = 0.736 这是训练集\n",
      "2021-04-23T00:41:24.595077: Epoch   3 Batch 2485/3125   train_loss = 0.820 这是训练集\n",
      "2021-04-23T00:41:25.207794: Epoch   3 Batch 2505/3125   train_loss = 0.827 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:41:25.856834: Epoch   3 Batch 2525/3125   train_loss = 0.859 这是训练集\n",
      "2021-04-23T00:41:26.474641: Epoch   3 Batch 2545/3125   train_loss = 1.041 这是训练集\n",
      "2021-04-23T00:41:27.086966: Epoch   3 Batch 2565/3125   train_loss = 0.909 这是训练集\n",
      "2021-04-23T00:41:27.691100: Epoch   3 Batch 2585/3125   train_loss = 0.813 这是训练集\n",
      "2021-04-23T00:41:28.272329: Epoch   3 Batch 2605/3125   train_loss = 0.856 这是训练集\n",
      "2021-04-23T00:41:28.853050: Epoch   3 Batch 2625/3125   train_loss = 0.962 这是训练集\n",
      "2021-04-23T00:41:29.447210: Epoch   3 Batch 2645/3125   train_loss = 0.948 这是训练集\n",
      "2021-04-23T00:41:30.047287: Epoch   3 Batch 2665/3125   train_loss = 0.982 这是训练集\n",
      "2021-04-23T00:41:30.595368: Epoch   3 Batch 2685/3125   train_loss = 0.888 这是训练集\n",
      "2021-04-23T00:41:31.192679: Epoch   3 Batch 2705/3125   train_loss = 0.824 这是训练集\n",
      "2021-04-23T00:41:31.779799: Epoch   3 Batch 2725/3125   train_loss = 0.967 这是训练集\n",
      "2021-04-23T00:41:32.364381: Epoch   3 Batch 2745/3125   train_loss = 0.932 这是训练集\n",
      "2021-04-23T00:41:32.943165: Epoch   3 Batch 2765/3125   train_loss = 0.777 这是训练集\n",
      "2021-04-23T00:41:33.541961: Epoch   3 Batch 2785/3125   train_loss = 1.013 这是训练集\n",
      "2021-04-23T00:41:34.166600: Epoch   3 Batch 2805/3125   train_loss = 0.854 这是训练集\n",
      "2021-04-23T00:41:34.773990: Epoch   3 Batch 2825/3125   train_loss = 0.841 这是训练集\n",
      "2021-04-23T00:41:35.474031: Epoch   3 Batch 2845/3125   train_loss = 0.865 这是训练集\n",
      "2021-04-23T00:41:36.119166: Epoch   3 Batch 2865/3125   train_loss = 0.835 这是训练集\n",
      "2021-04-23T00:41:36.689150: Epoch   3 Batch 2885/3125   train_loss = 0.906 这是训练集\n",
      "2021-04-23T00:41:37.301651: Epoch   3 Batch 2905/3125   train_loss = 0.969 这是训练集\n",
      "2021-04-23T00:41:37.924380: Epoch   3 Batch 2925/3125   train_loss = 0.909 这是训练集\n",
      "2021-04-23T00:41:38.488507: Epoch   3 Batch 2945/3125   train_loss = 0.928 这是训练集\n",
      "2021-04-23T00:41:39.091593: Epoch   3 Batch 2965/3125   train_loss = 0.937 这是训练集\n",
      "2021-04-23T00:41:39.705497: Epoch   3 Batch 2985/3125   train_loss = 0.804 这是训练集\n",
      "2021-04-23T00:41:40.350632: Epoch   3 Batch 3005/3125   train_loss = 0.836 这是训练集\n",
      "2021-04-23T00:41:40.947944: Epoch   3 Batch 3025/3125   train_loss = 0.921 这是训练集\n",
      "2021-04-23T00:41:41.555513: Epoch   3 Batch 3045/3125   train_loss = 0.955 这是训练集\n",
      "2021-04-23T00:41:42.180548: Epoch   3 Batch 3065/3125   train_loss = 0.858 这是训练集\n",
      "2021-04-23T00:41:42.729464: Epoch   3 Batch 3085/3125   train_loss = 0.866 这是训练集\n",
      "2021-04-23T00:41:43.350281: Epoch   3 Batch 3105/3125   train_loss = 0.894 这是训练集\n",
      "2021-04-23T00:41:44.256009: Epoch   3 Batch   17/781   test_loss = 0.924 这是测试集 \n",
      "2021-04-23T00:41:44.426812: Epoch   3 Batch   37/781   test_loss = 0.915 这是测试集 \n",
      "2021-04-23T00:41:44.616031: Epoch   3 Batch   57/781   test_loss = 0.970 这是测试集 \n",
      "2021-04-23T00:41:44.852223: Epoch   3 Batch   77/781   test_loss = 0.884 这是测试集 \n",
      "2021-04-23T00:41:45.075726: Epoch   3 Batch   97/781   test_loss = 0.731 这是测试集 \n",
      "2021-04-23T00:41:45.283614: Epoch   3 Batch  117/781   test_loss = 0.995 这是测试集 \n",
      "2021-04-23T00:41:45.482718: Epoch   3 Batch  137/781   test_loss = 0.904 这是测试集 \n",
      "2021-04-23T00:41:45.679869: Epoch   3 Batch  157/781   test_loss = 0.942 这是测试集 \n",
      "2021-04-23T00:41:45.879949: Epoch   3 Batch  177/781   test_loss = 0.910 这是测试集 \n",
      "2021-04-23T00:41:46.120047: Epoch   3 Batch  197/781   test_loss = 0.884 这是测试集 \n",
      "2021-04-23T00:41:46.321609: Epoch   3 Batch  217/781   test_loss = 0.728 这是测试集 \n",
      "2021-04-23T00:41:46.493891: Epoch   3 Batch  237/781   test_loss = 0.764 这是测试集 \n",
      "2021-04-23T00:41:46.691043: Epoch   3 Batch  257/781   test_loss = 1.063 这是测试集 \n",
      "2021-04-23T00:41:46.896003: Epoch   3 Batch  277/781   test_loss = 0.955 这是测试集 \n",
      "2021-04-23T00:41:47.113387: Epoch   3 Batch  297/781   test_loss = 0.976 这是测试集 \n",
      "2021-04-23T00:41:47.312471: Epoch   3 Batch  317/781   test_loss = 1.058 这是测试集 \n",
      "2021-04-23T00:41:47.538825: Epoch   3 Batch  337/781   test_loss = 0.899 这是测试集 \n",
      "2021-04-23T00:41:47.776969: Epoch   3 Batch  357/781   test_loss = 0.942 这是测试集 \n",
      "2021-04-23T00:41:47.993640: Epoch   3 Batch  377/781   test_loss = 0.983 这是测试集 \n",
      "2021-04-23T00:41:48.193719: Epoch   3 Batch  397/781   test_loss = 0.963 这是测试集 \n",
      "2021-04-23T00:41:48.383066: Epoch   3 Batch  417/781   test_loss = 0.829 这是测试集 \n",
      "2021-04-23T00:41:48.561672: Epoch   3 Batch  437/781   test_loss = 0.791 这是测试集 \n",
      "2021-04-23T00:41:48.757379: Epoch   3 Batch  457/781   test_loss = 0.743 这是测试集 \n",
      "2021-04-23T00:41:48.964290: Epoch   3 Batch  477/781   test_loss = 0.949 这是测试集 \n",
      "2021-04-23T00:41:49.205362: Epoch   3 Batch  497/781   test_loss = 0.839 这是测试集 \n",
      "2021-04-23T00:41:49.438160: Epoch   3 Batch  517/781   test_loss = 0.852 这是测试集 \n",
      "2021-04-23T00:41:49.645072: Epoch   3 Batch  537/781   test_loss = 0.895 这是测试集 \n",
      "2021-04-23T00:41:49.847104: Epoch   3 Batch  557/781   test_loss = 1.032 这是测试集 \n",
      "2021-04-23T00:41:50.052064: Epoch   3 Batch  577/781   test_loss = 0.923 这是测试集 \n",
      "2021-04-23T00:41:50.249215: Epoch   3 Batch  597/781   test_loss = 0.838 这是测试集 \n",
      "2021-04-23T00:41:50.429775: Epoch   3 Batch  617/781   test_loss = 0.886 这是测试集 \n",
      "2021-04-23T00:41:50.611311: Epoch   3 Batch  637/781   test_loss = 0.824 这是测试集 \n",
      "2021-04-23T00:41:50.804560: Epoch   3 Batch  657/781   test_loss = 1.047 这是测试集 \n",
      "2021-04-23T00:41:51.003545: Epoch   3 Batch  677/781   test_loss = 0.947 这是测试集 \n",
      "2021-04-23T00:41:51.212352: Epoch   3 Batch  697/781   test_loss = 0.990 这是测试集 \n",
      "2021-04-23T00:41:51.421258: Epoch   3 Batch  717/781   test_loss = 0.860 这是测试集 \n",
      "2021-04-23T00:41:51.628770: Epoch   3 Batch  737/781   test_loss = 0.772 这是测试集 \n",
      "2021-04-23T00:41:51.841834: Epoch   3 Batch  757/781   test_loss = 1.064 这是测试集 \n",
      "2021-04-23T00:41:52.039963: Epoch   3 Batch  777/781   test_loss = 0.966 这是测试集 \n",
      "2021-04-23T00:41:53.147724: Epoch   4 Batch    0/3125   train_loss = 0.982 这是训练集\n",
      "2021-04-23T00:41:53.728952: Epoch   4 Batch   20/3125   train_loss = 0.839 这是训练集\n",
      "2021-04-23T00:41:54.329191: Epoch   4 Batch   40/3125   train_loss = 0.896 这是训练集\n",
      "2021-04-23T00:41:54.914361: Epoch   4 Batch   60/3125   train_loss = 0.744 这是训练集\n",
      "2021-04-23T00:41:55.510697: Epoch   4 Batch   80/3125   train_loss = 0.851 这是训练集\n",
      "2021-04-23T00:41:56.146618: Epoch   4 Batch  100/3125   train_loss = 0.933 这是训练集\n",
      "2021-04-23T00:41:56.773159: Epoch   4 Batch  120/3125   train_loss = 0.987 这是训练集\n",
      "2021-04-23T00:41:57.390028: Epoch   4 Batch  140/3125   train_loss = 0.934 这是训练集\n",
      "2021-04-23T00:41:58.018572: Epoch   4 Batch  160/3125   train_loss = 0.825 这是训练集\n",
      "2021-04-23T00:41:58.591491: Epoch   4 Batch  180/3125   train_loss = 0.854 这是训练集\n",
      "2021-04-23T00:41:59.226360: Epoch   4 Batch  200/3125   train_loss = 1.139 这是训练集\n",
      "2021-04-23T00:41:59.851975: Epoch   4 Batch  220/3125   train_loss = 0.905 这是训练集\n",
      "2021-04-23T00:42:00.477124: Epoch   4 Batch  240/3125   train_loss = 0.947 这是训练集\n",
      "2021-04-23T00:42:01.106683: Epoch   4 Batch  260/3125   train_loss = 0.942 这是训练集\n",
      "2021-04-23T00:42:01.703019: Epoch   4 Batch  280/3125   train_loss = 0.964 这是训练集\n",
      "2021-04-23T00:42:02.327188: Epoch   4 Batch  300/3125   train_loss = 1.102 这是训练集\n",
      "2021-04-23T00:42:02.947173: Epoch   4 Batch  320/3125   train_loss = 0.952 这是训练集\n",
      "2021-04-23T00:42:03.617765: Epoch   4 Batch  340/3125   train_loss = 0.778 这是训练集\n",
      "2021-04-23T00:42:04.304869: Epoch   4 Batch  360/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:42:04.989040: Epoch   4 Batch  380/3125   train_loss = 0.896 这是训练集\n",
      "2021-04-23T00:42:05.680189: Epoch   4 Batch  400/3125   train_loss = 0.867 这是训练集\n",
      "2021-04-23T00:42:06.341916: Epoch   4 Batch  420/3125   train_loss = 0.807 这是训练集\n",
      "2021-04-23T00:42:06.913700: Epoch   4 Batch  440/3125   train_loss = 0.870 这是训练集\n",
      "2021-04-23T00:42:07.627557: Epoch   4 Batch  460/3125   train_loss = 0.881 这是训练集\n",
      "2021-04-23T00:42:08.282529: Epoch   4 Batch  480/3125   train_loss = 0.947 这是训练集\n",
      "2021-04-23T00:42:08.842752: Epoch   4 Batch  500/3125   train_loss = 0.712 这是训练集\n",
      "2021-04-23T00:42:09.473287: Epoch   4 Batch  520/3125   train_loss = 0.914 这是训练集\n",
      "2021-04-23T00:42:10.132826: Epoch   4 Batch  540/3125   train_loss = 0.826 这是训练集\n",
      "2021-04-23T00:42:10.772106: Epoch   4 Batch  560/3125   train_loss = 0.970 这是训练集\n",
      "2021-04-23T00:42:11.389914: Epoch   4 Batch  580/3125   train_loss = 0.964 这是训练集\n",
      "2021-04-23T00:42:11.999205: Epoch   4 Batch  600/3125   train_loss = 0.936 这是训练集\n",
      "2021-04-23T00:42:12.571537: Epoch   4 Batch  620/3125   train_loss = 0.904 这是训练集\n",
      "2021-04-23T00:42:13.182512: Epoch   4 Batch  640/3125   train_loss = 0.886 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:42:13.780332: Epoch   4 Batch  660/3125   train_loss = 0.926 这是训练集\n",
      "2021-04-23T00:42:14.369561: Epoch   4 Batch  680/3125   train_loss = 0.959 这是训练集\n",
      "2021-04-23T00:42:14.950141: Epoch   4 Batch  700/3125   train_loss = 0.931 这是训练集\n",
      "2021-04-23T00:42:15.574351: Epoch   4 Batch  720/3125   train_loss = 0.826 这是训练集\n",
      "2021-04-23T00:42:16.224367: Epoch   4 Batch  740/3125   train_loss = 0.963 这是训练集\n",
      "2021-04-23T00:42:16.817107: Epoch   4 Batch  760/3125   train_loss = 0.810 这是训练集\n",
      "2021-04-23T00:42:17.449271: Epoch   4 Batch  780/3125   train_loss = 0.937 这是训练集\n",
      "2021-04-23T00:42:18.050994: Epoch   4 Batch  800/3125   train_loss = 0.828 这是训练集\n",
      "2021-04-23T00:42:18.640498: Epoch   4 Batch  820/3125   train_loss = 0.859 这是训练集\n",
      "2021-04-23T00:42:19.306215: Epoch   4 Batch  840/3125   train_loss = 0.799 这是训练集\n",
      "2021-04-23T00:42:19.902708: Epoch   4 Batch  860/3125   train_loss = 0.873 这是训练集\n",
      "2021-04-23T00:42:20.475648: Epoch   4 Batch  880/3125   train_loss = 0.839 这是训练集\n",
      "2021-04-23T00:42:21.079791: Epoch   4 Batch  900/3125   train_loss = 0.860 这是训练集\n",
      "2021-04-23T00:42:21.699589: Epoch   4 Batch  920/3125   train_loss = 0.978 这是训练集\n",
      "2021-04-23T00:42:22.340629: Epoch   4 Batch  940/3125   train_loss = 0.856 这是训练集\n",
      "2021-04-23T00:42:22.947163: Epoch   4 Batch  960/3125   train_loss = 0.880 这是训练集\n",
      "2021-04-23T00:42:23.570828: Epoch   4 Batch  980/3125   train_loss = 0.968 这是训练集\n",
      "2021-04-23T00:42:24.204251: Epoch   4 Batch 1000/3125   train_loss = 0.926 这是训练集\n",
      "2021-04-23T00:42:24.826151: Epoch   4 Batch 1020/3125   train_loss = 0.892 这是训练集\n",
      "2021-04-23T00:42:25.449814: Epoch   4 Batch 1040/3125   train_loss = 0.777 这是训练集\n",
      "2021-04-23T00:42:26.084214: Epoch   4 Batch 1060/3125   train_loss = 0.997 这是训练集\n",
      "2021-04-23T00:42:26.655699: Epoch   4 Batch 1080/3125   train_loss = 0.911 这是训练集\n",
      "2021-04-23T00:42:27.260392: Epoch   4 Batch 1100/3125   train_loss = 0.853 这是训练集\n",
      "2021-04-23T00:42:27.923096: Epoch   4 Batch 1120/3125   train_loss = 0.872 这是训练集\n",
      "2021-04-23T00:42:28.496515: Epoch   4 Batch 1140/3125   train_loss = 0.872 这是训练集\n",
      "2021-04-23T00:42:29.101992: Epoch   4 Batch 1160/3125   train_loss = 0.798 这是训练集\n",
      "2021-04-23T00:42:29.706242: Epoch   4 Batch 1180/3125   train_loss = 0.843 这是训练集\n",
      "2021-04-23T00:42:30.304530: Epoch   4 Batch 1200/3125   train_loss = 0.991 这是训练集\n",
      "2021-04-23T00:42:30.930418: Epoch   4 Batch 1220/3125   train_loss = 0.972 这是训练集\n",
      "2021-04-23T00:42:31.540418: Epoch   4 Batch 1240/3125   train_loss = 0.786 这是训练集\n",
      "2021-04-23T00:42:32.179697: Epoch   4 Batch 1260/3125   train_loss = 0.885 这是训练集\n",
      "2021-04-23T00:42:32.781932: Epoch   4 Batch 1280/3125   train_loss = 0.927 这是训练集\n",
      "2021-04-23T00:42:33.397790: Epoch   4 Batch 1300/3125   train_loss = 0.830 这是训练集\n",
      "2021-04-23T00:42:34.006851: Epoch   4 Batch 1320/3125   train_loss = 0.876 这是训练集\n",
      "2021-04-23T00:42:34.610994: Epoch   4 Batch 1340/3125   train_loss = 0.748 这是训练集\n",
      "2021-04-23T00:42:35.202487: Epoch   4 Batch 1360/3125   train_loss = 0.812 这是训练集\n",
      "2021-04-23T00:42:35.840974: Epoch   4 Batch 1380/3125   train_loss = 0.818 这是训练集\n",
      "2021-04-23T00:42:36.435749: Epoch   4 Batch 1400/3125   train_loss = 0.945 这是训练集\n",
      "2021-04-23T00:42:37.046724: Epoch   4 Batch 1420/3125   train_loss = 0.937 这是训练集\n",
      "2021-04-23T00:42:37.691391: Epoch   4 Batch 1440/3125   train_loss = 0.766 这是训练集\n",
      "2021-04-23T00:42:38.309137: Epoch   4 Batch 1460/3125   train_loss = 0.885 这是训练集\n",
      "2021-04-23T00:42:38.928935: Epoch   4 Batch 1480/3125   train_loss = 0.916 这是训练集\n",
      "2021-04-23T00:42:39.578950: Epoch   4 Batch 1500/3125   train_loss = 0.914 这是训练集\n",
      "2021-04-23T00:42:40.223617: Epoch   4 Batch 1520/3125   train_loss = 0.803 这是训练集\n",
      "2021-04-23T00:42:40.789062: Epoch   4 Batch 1540/3125   train_loss = 0.962 这是训练集\n",
      "2021-04-23T00:42:41.384423: Epoch   4 Batch 1560/3125   train_loss = 0.791 这是训练集\n",
      "2021-04-23T00:42:42.028581: Epoch   4 Batch 1580/3125   train_loss = 1.000 这是训练集\n",
      "2021-04-23T00:42:42.609458: Epoch   4 Batch 1600/3125   train_loss = 0.781 这是训练集\n",
      "2021-04-23T00:42:43.244422: Epoch   4 Batch 1620/3125   train_loss = 0.807 这是训练集\n",
      "2021-04-23T00:42:43.844662: Epoch   4 Batch 1640/3125   train_loss = 0.943 这是训练集\n",
      "2021-04-23T00:42:44.436158: Epoch   4 Batch 1660/3125   train_loss = 0.987 这是训练集\n",
      "2021-04-23T00:42:45.070231: Epoch   4 Batch 1680/3125   train_loss = 0.886 这是训练集\n",
      "2021-04-23T00:42:45.702289: Epoch   4 Batch 1700/3125   train_loss = 0.824 这是训练集\n",
      "2021-04-23T00:42:46.355233: Epoch   4 Batch 1720/3125   train_loss = 0.921 这是训练集\n",
      "2021-04-23T00:42:46.974016: Epoch   4 Batch 1740/3125   train_loss = 0.946 这是训练集\n",
      "2021-04-23T00:42:47.615795: Epoch   4 Batch 1760/3125   train_loss = 0.897 这是训练集\n",
      "2021-04-23T00:42:48.241917: Epoch   4 Batch 1780/3125   train_loss = 0.880 这是训练集\n",
      "2021-04-23T00:42:48.849965: Epoch   4 Batch 1800/3125   train_loss = 0.829 这是训练集\n",
      "2021-04-23T00:42:49.493655: Epoch   4 Batch 1820/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:42:50.395773: Epoch   4 Batch 1840/3125   train_loss = 0.907 这是训练集\n",
      "2021-04-23T00:42:51.004797: Epoch   4 Batch 1860/3125   train_loss = 0.965 这是训练集\n",
      "2021-04-23T00:42:51.632925: Epoch   4 Batch 1880/3125   train_loss = 0.873 这是训练集\n",
      "2021-04-23T00:42:52.265618: Epoch   4 Batch 1900/3125   train_loss = 0.799 这是训练集\n",
      "2021-04-23T00:42:52.885631: Epoch   4 Batch 1920/3125   train_loss = 0.878 这是训练集\n",
      "2021-04-23T00:42:53.539043: Epoch   4 Batch 1940/3125   train_loss = 0.807 这是训练集\n",
      "2021-04-23T00:42:54.203232: Epoch   4 Batch 1960/3125   train_loss = 0.761 这是训练集\n",
      "2021-04-23T00:42:54.784460: Epoch   4 Batch 1980/3125   train_loss = 0.881 这是训练集\n",
      "2021-04-23T00:42:55.382248: Epoch   4 Batch 2000/3125   train_loss = 1.057 这是训练集\n",
      "2021-04-23T00:42:55.993728: Epoch   4 Batch 2020/3125   train_loss = 0.961 这是训练集\n",
      "2021-04-23T00:42:56.548096: Epoch   4 Batch 2040/3125   train_loss = 0.764 这是训练集\n",
      "2021-04-23T00:42:57.215684: Epoch   4 Batch 2060/3125   train_loss = 0.823 这是训练集\n",
      "2021-04-23T00:42:57.886322: Epoch   4 Batch 2080/3125   train_loss = 1.004 这是训练集\n",
      "2021-04-23T00:42:58.548049: Epoch   4 Batch 2100/3125   train_loss = 0.820 这是训练集\n",
      "2021-04-23T00:42:59.179520: Epoch   4 Batch 2120/3125   train_loss = 0.799 这是训练集\n",
      "2021-04-23T00:42:59.772984: Epoch   4 Batch 2140/3125   train_loss = 0.878 这是训练集\n",
      "2021-04-23T00:43:00.432290: Epoch   4 Batch 2160/3125   train_loss = 0.875 这是训练集\n",
      "2021-04-23T00:43:01.053026: Epoch   4 Batch 2180/3125   train_loss = 0.918 这是训练集\n",
      "2021-04-23T00:43:01.733040: Epoch   4 Batch 2200/3125   train_loss = 0.838 这是训练集\n",
      "2021-04-23T00:43:02.391842: Epoch   4 Batch 2220/3125   train_loss = 0.821 这是训练集\n",
      "2021-04-23T00:43:02.959874: Epoch   4 Batch 2240/3125   train_loss = 0.851 这是训练集\n",
      "2021-04-23T00:43:03.558260: Epoch   4 Batch 2260/3125   train_loss = 0.894 这是训练集\n",
      "2021-04-23T00:43:04.215187: Epoch   4 Batch 2280/3125   train_loss = 0.878 这是训练集\n",
      "2021-04-23T00:43:04.859346: Epoch   4 Batch 2300/3125   train_loss = 0.812 这是训练集\n",
      "2021-04-23T00:43:05.591854: Epoch   4 Batch 2320/3125   train_loss = 0.975 这是训练集\n",
      "2021-04-23T00:43:06.217547: Epoch   4 Batch 2340/3125   train_loss = 0.871 这是训练集\n",
      "2021-04-23T00:43:06.786584: Epoch   4 Batch 2360/3125   train_loss = 0.869 这是训练集\n",
      "2021-04-23T00:43:07.395607: Epoch   4 Batch 2380/3125   train_loss = 0.847 这是训练集\n",
      "2021-04-23T00:43:08.023174: Epoch   4 Batch 2400/3125   train_loss = 0.919 这是训练集\n",
      "2021-04-23T00:43:08.623418: Epoch   4 Batch 2420/3125   train_loss = 0.781 这是训练集\n",
      "2021-04-23T00:43:09.230733: Epoch   4 Batch 2440/3125   train_loss = 0.859 这是训练集\n",
      "2021-04-23T00:43:09.837804: Epoch   4 Batch 2460/3125   train_loss = 0.880 这是训练集\n",
      "2021-04-23T00:43:10.434016: Epoch   4 Batch 2480/3125   train_loss = 0.992 这是训练集\n",
      "2021-04-23T00:43:11.053746: Epoch   4 Batch 2500/3125   train_loss = 0.831 这是训练集\n",
      "2021-04-23T00:43:11.699858: Epoch   4 Batch 2520/3125   train_loss = 0.953 这是训练集\n",
      "2021-04-23T00:43:12.318036: Epoch   4 Batch 2540/3125   train_loss = 0.828 这是训练集\n",
      "2021-04-23T00:43:12.912475: Epoch   4 Batch 2560/3125   train_loss = 0.670 这是训练集\n",
      "2021-04-23T00:43:13.513305: Epoch   4 Batch 2580/3125   train_loss = 0.870 这是训练集\n",
      "2021-04-23T00:43:14.102807: Epoch   4 Batch 2600/3125   train_loss = 0.863 这是训练集\n",
      "2021-04-23T00:43:14.670372: Epoch   4 Batch 2620/3125   train_loss = 0.815 这是训练集\n",
      "2021-04-23T00:43:15.287831: Epoch   4 Batch 2640/3125   train_loss = 0.860 这是训练集\n",
      "2021-04-23T00:43:15.894715: Epoch   4 Batch 2660/3125   train_loss = 0.981 这是训练集\n",
      "2021-04-23T00:43:16.483243: Epoch   4 Batch 2680/3125   train_loss = 0.824 这是训练集\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23T00:43:17.134272: Epoch   4 Batch 2700/3125   train_loss = 0.896 这是训练集\n",
      "2021-04-23T00:43:17.756419: Epoch   4 Batch 2720/3125   train_loss = 0.745 这是训练集\n",
      "2021-04-23T00:43:18.408895: Epoch   4 Batch 2740/3125   train_loss = 0.852 这是训练集\n",
      "2021-04-23T00:43:19.034512: Epoch   4 Batch 2760/3125   train_loss = 0.788 这是训练集\n",
      "2021-04-23T00:43:19.695815: Epoch   4 Batch 2780/3125   train_loss = 0.850 这是训练集\n",
      "2021-04-23T00:43:20.297259: Epoch   4 Batch 2800/3125   train_loss = 1.022 这是训练集\n",
      "2021-04-23T00:43:20.866266: Epoch   4 Batch 2820/3125   train_loss = 1.043 这是训练集\n",
      "2021-04-23T00:43:21.458697: Epoch   4 Batch 2840/3125   train_loss = 0.845 这是训练集\n",
      "2021-04-23T00:43:22.075063: Epoch   4 Batch 2860/3125   train_loss = 0.806 这是训练集\n",
      "2021-04-23T00:43:22.672312: Epoch   4 Batch 2880/3125   train_loss = 0.831 这是训练集\n",
      "2021-04-23T00:43:23.271186: Epoch   4 Batch 2900/3125   train_loss = 0.869 这是训练集\n",
      "2021-04-23T00:43:23.870449: Epoch   4 Batch 2920/3125   train_loss = 0.856 这是训练集\n",
      "2021-04-23T00:43:24.433600: Epoch   4 Batch 2940/3125   train_loss = 0.880 这是训练集\n",
      "2021-04-23T00:43:25.069500: Epoch   4 Batch 2960/3125   train_loss = 0.912 这是训练集\n",
      "2021-04-23T00:43:25.768811: Epoch   4 Batch 2980/3125   train_loss = 0.823 这是训练集\n",
      "2021-04-23T00:43:26.392473: Epoch   4 Batch 3000/3125   train_loss = 0.954 这是训练集\n",
      "2021-04-23T00:43:26.993693: Epoch   4 Batch 3020/3125   train_loss = 0.989 这是训练集\n",
      "2021-04-23T00:43:27.612440: Epoch   4 Batch 3040/3125   train_loss = 0.887 这是训练集\n",
      "2021-04-23T00:43:28.256893: Epoch   4 Batch 3060/3125   train_loss = 0.777 这是训练集\n",
      "2021-04-23T00:43:28.851277: Epoch   4 Batch 3080/3125   train_loss = 0.999 这是训练集\n",
      "2021-04-23T00:43:29.464713: Epoch   4 Batch 3100/3125   train_loss = 1.037 这是训练集\n",
      "2021-04-23T00:43:30.061434: Epoch   4 Batch 3120/3125   train_loss = 0.858 这是训练集\n",
      "2021-04-23T00:43:30.334714: Epoch   4 Batch   16/781   test_loss = 0.818 这是测试集 \n",
      "2021-04-23T00:43:30.512951: Epoch   4 Batch   36/781   test_loss = 0.949 这是测试集 \n",
      "2021-04-23T00:43:30.723083: Epoch   4 Batch   56/781   test_loss = 0.921 这是测试集 \n",
      "2021-04-23T00:43:30.942213: Epoch   4 Batch   76/781   test_loss = 0.959 这是测试集 \n",
      "2021-04-23T00:43:31.154981: Epoch   4 Batch   96/781   test_loss = 1.000 这是测试集 \n",
      "2021-04-23T00:43:31.351156: Epoch   4 Batch  116/781   test_loss = 0.873 这是测试集 \n",
      "2021-04-23T00:43:31.551237: Epoch   4 Batch  136/781   test_loss = 0.841 这是测试集 \n",
      "2021-04-23T00:43:31.785476: Epoch   4 Batch  156/781   test_loss = 0.857 这是测试集 \n",
      "2021-04-23T00:43:32.019718: Epoch   4 Batch  176/781   test_loss = 0.899 这是测试集 \n",
      "2021-04-23T00:43:32.219796: Epoch   4 Batch  196/781   test_loss = 0.781 这是测试集 \n",
      "2021-04-23T00:43:32.401335: Epoch   4 Batch  216/781   test_loss = 1.000 这是测试集 \n",
      "2021-04-23T00:43:32.599463: Epoch   4 Batch  236/781   test_loss = 0.792 这是测试集 \n",
      "2021-04-23T00:43:32.831929: Epoch   4 Batch  256/781   test_loss = 0.813 这是测试集 \n",
      "2021-04-23T00:43:33.061101: Epoch   4 Batch  276/781   test_loss = 1.068 这是测试集 \n",
      "2021-04-23T00:43:33.285582: Epoch   4 Batch  296/781   test_loss = 0.852 这是测试集 \n",
      "2021-04-23T00:43:33.499324: Epoch   4 Batch  316/781   test_loss = 0.848 这是测试集 \n",
      "2021-04-23T00:43:33.719901: Epoch   4 Batch  336/781   test_loss = 0.741 这是测试集 \n",
      "2021-04-23T00:43:33.930715: Epoch   4 Batch  356/781   test_loss = 0.875 这是测试集 \n",
      "2021-04-23T00:43:34.137628: Epoch   4 Batch  376/781   test_loss = 0.903 这是测试集 \n",
      "2021-04-23T00:43:34.318700: Epoch   4 Batch  396/781   test_loss = 0.891 这是测试集 \n",
      "2021-04-23T00:43:34.497794: Epoch   4 Batch  416/781   test_loss = 0.953 这是测试集 \n",
      "2021-04-23T00:43:34.680157: Epoch   4 Batch  436/781   test_loss = 0.905 这是测试集 \n",
      "2021-04-23T00:43:34.878285: Epoch   4 Batch  456/781   test_loss = 0.766 这是测试集 \n",
      "2021-04-23T00:43:35.070738: Epoch   4 Batch  476/781   test_loss = 0.941 这是测试集 \n",
      "2021-04-23T00:43:35.286433: Epoch   4 Batch  496/781   test_loss = 0.972 这是测试集 \n",
      "2021-04-23T00:43:35.514818: Epoch   4 Batch  516/781   test_loss = 0.785 这是测试集 \n",
      "2021-04-23T00:43:35.738321: Epoch   4 Batch  536/781   test_loss = 0.939 这是测试集 \n",
      "2021-04-23T00:43:35.930592: Epoch   4 Batch  556/781   test_loss = 0.800 这是测试集 \n",
      "2021-04-23T00:43:36.130672: Epoch   4 Batch  576/781   test_loss = 1.005 这是测试集 \n",
      "2021-04-23T00:43:36.316112: Epoch   4 Batch  596/781   test_loss = 0.964 这是测试集 \n",
      "2021-04-23T00:43:36.489052: Epoch   4 Batch  616/781   test_loss = 0.950 这是测试集 \n",
      "2021-04-23T00:43:36.687181: Epoch   4 Batch  636/781   test_loss = 0.881 这是测试集 \n",
      "2021-04-23T00:43:36.884928: Epoch   4 Batch  656/781   test_loss = 0.874 这是测试集 \n",
      "2021-04-23T00:43:37.079953: Epoch   4 Batch  676/781   test_loss = 1.040 这是测试集 \n",
      "2021-04-23T00:43:37.282962: Epoch   4 Batch  696/781   test_loss = 0.855 这是测试集 \n",
      "2021-04-23T00:43:37.488897: Epoch   4 Batch  716/781   test_loss = 0.928 这是测试集 \n",
      "2021-04-23T00:43:37.704592: Epoch   4 Batch  736/781   test_loss = 1.084 这是测试集 \n",
      "2021-04-23T00:43:37.902720: Epoch   4 Batch  756/781   test_loss = 0.818 这是测试集 \n",
      "2021-04-23T00:43:38.127203: Epoch   4 Batch  776/781   test_loss = 0.763 这是测试集 \n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    " #ipython notebook中有一个相当方便的语句: %matplotlib inline，可以实现运行cell即出现结果图像。\n",
    " #但是如果想写在Python程序内，显示图像，需要加上一句: plt.show()。\n",
    " #当然确保导入了对应的库： import matplotlib.pyplot as plt\n",
    "                 \n",
    " #魔法函数（Magic Functions）分两种：一种是面向行的，另一种是面向单元型的。\n",
    " #行magic函数：用前缀“%”标注的,很像使用命令行时的形式,“%”后面就是magic函数的参数了,但参数是没有被写在括号或者引号中来传值的\n",
    " #单元型magic函数是由两个“%%”做前缀的，它的参数不仅是当前“%%”行后面的内容，也包括了在当前行以下的行\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "\n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    #获得你要得知的运算结果, 或者是你所要运算的部分.\n",
    "    \n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            #直方图\n",
    "            #tf.summary.histogram('summary_name', tensor)用来显示直方图信息\n",
    "            #将【计算图】中的【数据的分布/数据直方图】写入TensorFlow中的【日志文件】，以便为将来tensorboard的可视化做准备\n",
    "            #一般用来显示训练过程中变量的分布情况\n",
    "\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            #显示标量信息                                                                 有效衡量relu激活函数的有效性\n",
    "            #tf.summary.scalar(name, tensor, collections=None) 用来显示标量信息，一般在画loss,accuary时会用到这个函数\n",
    "            #将【计算图】中的【标量数据】写入TensorFlow中的【日志文件】，以便为将来tensorboard的可视化做准备\n",
    "            #tf.nn.zero_fraction统计某个值的0的比例，这个tf.nn.zero_fraction计算出来的值越大，0的比例越高，稀疏度\n",
    "\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries) #对各类的汇总进行一次合并\n",
    "    #tf.summary.merge(inputs, collections=None, name=None)将上面几种类型的汇总再进行一次合并，具体合并哪些由inputs指定\n",
    "    # merge_all将之前定义的所有summary整合在一起\n",
    "    #[2]和TensorFlow中的其他操作类似，tf.summary.scalar、tf.summary.histogram、tf.summary.image函数也是一个op\n",
    "    #在定义的时候，也不会立即执行，需要通过sess.run来明确调用这些函数。因为，在一个程序中定义的写日志操作比较多\n",
    "    #如果一一调用，将会十分麻烦，所以Tensorflow提供了tf.summary.merge_all()函数将所有的summary整理在一起\n",
    "    #在TensorFlow程序执行的时候，只需要运行这一个操作就可以将代码中定义的所有【写日志操作】执行一次，从而将所有的日志写入【日志文件】。\n",
    "\n",
    "\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    #os.path.curdir:当前目录\n",
    "    #os.path.join(): 常用来链接路径\n",
    "    #os.path.abspath 绝对路径\n",
    "    #Python中有join()和os.path.join()两个函数\n",
    "    #join()： 连接字符串数组。将字符串、元组、列表中的元素以指定的字符(分隔符)连接生成一个新的字符串 \n",
    "    #os.path.join()： 将多个路径组合后返回:os.path.join(path1[,path2[,……]])\n",
    "    #第一个以”/”开头的参数开始拼接，之前的参数全部丢弃\n",
    "    #以上一种情况为先。在上一种情况确保情况下，若出现”./”开头的参数，会从”./”开头的参数的上一个参数开始拼接\n",
    "    #os.path.join('aaaa','/bbbb','ccccc.txt')--> /bbbb\\ccccc.txt只有一个以”/”开头的，参数从它开始往后拼接，之前的参数全部丢弃\n",
    "    #os.path.join('/aaaa','/bbbb','/ccccc.txt')-->/ccccc.txt有多个以”/”开头的参数，从最后”/”开头的的开始往后拼接，之前的参数全部丢弃\n",
    "    #os.path.join('aaaa','./bbb','ccccc.txt')-->aaaa\\./bbb\\ccccc.txt若出现”./”开头的参数，会从”./”开头的参数的上一个参数开始拼接,即只包含上一个参数加往后的\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy  显示标量\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        # 指定一个文件用来保存图。   path,sess.graph \n",
    "        #定义一个写入summary的目标文件，dir为写入文件地址 \n",
    "        #tf.summary.FileWritter(path,sess.graph)指定一个文件用来保存图\n",
    "        \n",
    "        \n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())#参数的初始化\n",
    "    saver = tf.train.Saver()  #用于后面保存数据，创建一个saver对象\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        #将数据集分成训练集和测试集，随机种子不固定   模板\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "\n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "\n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size): # // \"表示整数除法\n",
    "            x, y = next(train_batches)\n",
    "            #next() 返回迭代器的下一个项目，next(get_batches(train_X, train_y, batch_size))\n",
    "            #在这个for循环每次都进行上一个batch的后面一个batch\n",
    "\n",
    "            categories = np.zeros([batch_size, 18]) #返回来一个给定形状和类型的用0填充的数组\n",
    "            for i in range(batch_size):  \n",
    "                categories[i] = x.take(6,1)[i]  #沿轴取数组中的元素\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #调用train_writer的add_summary方法将训练过程以及训练步数保存\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat() #此方法的返回类型是日期的ISO 8601格式的字符串\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f} 这是训练集'.format(\n",
    "                    time_str,       #> 又对其  <左对齐     后面数字是宽度\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "\n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)  #next() 返回迭代器的下一个项目。\n",
    "\n",
    "            categories = np.zeros([batch_size, 18]) #返回来一个给定形状和类型的用0填充的数组\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]  #据提供的索引值将元素形成数组输出\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #调用train_writer的add_summary方法将训练过程以及训练步数保存\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f} 这是测试集 '.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pharmaceutical-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "monthly-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vertical-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hundred-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") #之前是MatMul:0 因为inference代码修改了\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liberal-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "equal-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.6764371]], dtype=float32)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "complimentary-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fatty-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "terminal-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        #movie_matrics显示为 (3883, 200)\n",
    "        #array([[-0.9784413 ,  0.97033578, -0.99996817, ..., -0.94367135,0.938721  ,  0.94092846],...])\n",
    "        #tf.square()是对a里的每一个元素求平方i=(x,y)\n",
    "        #tf.reduce_sum,注意参数表示在维度1(列)上进行求和,且维度不变 x^2+y^2\n",
    "        #tf.sqrt计算x元素的平方根\n",
    "        #这里完成向量单位化\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics # / 表示浮点数除法,返回浮点结果\n",
    "                                #单位化后(  x/(x^2+y^2),y/(x^2+y^2)  )\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        #用户输入已看过的电影，进行movieid2idx数字转化\n",
    "        #movie_matrics[转化后的标记数值]得到对应的电影特征向量\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        #矩阵乘法(x1,x2)和(y1,y2)可以得到x1y1+x2y2\n",
    "        #即得到输入的电影与各个电影的余弦相似性的值\n",
    "        #(1,200)×(200,3883)\n",
    "        sim = (probs_similarity.eval()) #转化为字符串 执行一个字符串表达式，并返回表达式的值。\n",
    "         #sim [[ 13.49374485  13.48943233  13.51107979 ...,  13.50281906  13.49236774  13.49707603]]\n",
    "        \n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]])) #movies_orig原始未处理的电影数据，为输出用户可读\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim) #np.squeeze将表示向量的数组转换为秩为1的数组\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        #numpy.argsort()\n",
    "        #x=np.array([1,4,3,-1,6,9])\n",
    "        #函数含义：首先将p中的元素从小到大排列后，得到[-1,1,3,4,6,9]\n",
    "        #按照所得的排好序的对应找其在原x中的索引值，如-1由x[3]得到；1由x[0]得到，所以索引值为[3,0,2,1,4,5]\n",
    "        #所以这个即为输出\n",
    "        #np.argsort()[:-top_k]表示将np.argsort()得到的结果去掉后面20个后的前面所有值为0，因为我们只考虑最相似的20个\n",
    "        #这些值不为0，以便做后面的处理\n",
    "        \n",
    "        \n",
    "        p = p / np.sum(p) #sum函数对某一维度求和，这里表示全部元素求和,这里将p的值限制在0~1\n",
    "        results = set()\n",
    "        while len(results) != 5:           \n",
    "            c = np.random.choice(3883, 1, p=p)[0]#参数意思分别 是从a 中以概率P，随机选择5个,\n",
    "                                #p没有指定的时候表示同等概率会被取出，p指定时表示每个数会被取出的概率\n",
    "                                #replace代表的意思是抽样之后不放回，选出的5个数都不一样\n",
    "            results.add(c) #results本身为set（可以完成剔除掉相同的推荐，虽然前面np.random.choice是不放回）\n",
    "        for val in (results): \n",
    "            print(val)  #由于前面已经转换为字符串eval，所以可以直接输出\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "worldwide-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "864\n",
      "[875 'Nothing to Lose (1994)' 'Drama']\n",
      "2793\n",
      "[2862 'Caligula (1980)' 'Drama']\n",
      "1746\n",
      "[1810 'Primary Colors (1998)' 'Drama']\n",
      "1771\n",
      "[1840 'He Got Game (1998)' 'Drama']\n",
      "3219\n",
      "[3288 'Cotton Mary (1999)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{864, 1746, 1771, 2793, 3219}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "together-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "logical-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "以下是给您的推荐：\n",
      "736\n",
      "[746 'Force of Evil (1948)' 'Film-Noir']\n",
      "49\n",
      "[50 'Usual Suspects, The (1995)' 'Crime|Thriller']\n",
      "315\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n",
      "892\n",
      "[904 'Rear Window (1954)' 'Mystery|Thriller']\n",
      "763\n",
      "[773 'Touki Bouki (Journey of the Hyena) (1973)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{49, 315, 736, 763, 892}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-detective",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
